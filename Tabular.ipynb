{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, BatchSampler\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import shap \n",
    "print(f\"SHAP version: {shap.__version__}\")\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import collections\n",
    "\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Input, Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLayer1d(nn.Module):\n",
    "\n",
    "    def __init__(self, value, append):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "        self.append = append\n",
    "\n",
    "    def forward(self, input_tuple):\n",
    "        x, S = input_tuple\n",
    "        x = x * S + self.value * (1 - S)\n",
    "        if self.append:\n",
    "            x = torch.cat((x, S), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaskLayer2d(nn.Module):\n",
    "\n",
    "    def __init__(self, value, append):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "        self.append = append\n",
    "\n",
    "    def forward(self, input_tuple):\n",
    "        x, S = input_tuple\n",
    "        if len(S.shape) == 3:\n",
    "            S = S.unsqueeze(1)\n",
    "        x = x * S + self.value * (1 - S)\n",
    "        if self.append:\n",
    "            x = torch.cat((x, S), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class KLDivLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, reduction='batchmean', log_target=False):\n",
    "        super().__init__()\n",
    "        self.kld = nn.KLDivLoss(reduction=reduction, log_target=log_target)\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "\n",
    "        return self.kld(pred.log_softmax(dim=1), target)  #####################################\n",
    "\n",
    "\n",
    "class DatasetRepeat(Dataset):\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        # Get maximum number of elements.\n",
    "        assert np.all([isinstance(dset, Dataset) for dset in datasets])\n",
    "        items = [len(dset) for dset in datasets]\n",
    "        num_items = np.max(items)\n",
    "\n",
    "        # Ensure all datasets align.\n",
    "        # assert np.all([num_items % num == 0 for num in items])\n",
    "        self.dsets = datasets\n",
    "        self.num_items = num_items\n",
    "        self.items = items\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert 0 <= index < self.num_items\n",
    "        return_items = [dset[index % num] for dset, num in\n",
    "                        zip(self.dsets, self.items)]\n",
    "        return tuple(itertools.chain(*return_items))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_items\n",
    "\n",
    "\n",
    "class DatasetInputOnly(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        assert isinstance(dataset, Dataset)\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.dataset[index][0],)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "class UniformSampler:\n",
    "\n",
    "    def __init__(self, num_players):\n",
    "        self.num_players = num_players\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        S = torch.ones(batch_size, self.num_players, dtype=torch.float32)\n",
    "        num_included = (torch.rand(batch_size) * (self.num_players + 1)).int()\n",
    "        # TODO ideally avoid for loops\n",
    "        # TODO ideally pass buffer to assign samples in place\n",
    "        for i in range(batch_size):\n",
    "            S[i, num_included[i]:] = 0\n",
    "            S[i] = S[i, torch.randperm(self.num_players)]\n",
    "\n",
    "        return S\n",
    "\n",
    "\n",
    "class ShapleySampler:\n",
    "\n",
    "    def __init__(self, num_players):\n",
    "        arange = torch.arange(1, num_players)\n",
    "        w = 1 / (arange * (num_players - arange))\n",
    "        w = w / torch.sum(w)\n",
    "        self.categorical = Categorical(probs=w)\n",
    "        self.num_players = num_players\n",
    "        self.tril = torch.tril(\n",
    "            torch.ones(num_players - 1, num_players, dtype=torch.float32),\n",
    "            diagonal=0)\n",
    "\n",
    "    def sample(self, batch_size, paired_sampling):\n",
    "\n",
    "        num_included = 1 + self.categorical.sample([batch_size])\n",
    "        S = self.tril[num_included - 1]\n",
    "        # TODO ideally avoid for loops\n",
    "        for i in range(batch_size):\n",
    "            if paired_sampling and i % 2 == 1:\n",
    "                S[i] = 1 - S[i - 1]\n",
    "            else:\n",
    "                S[i] = S[i, torch.randperm(self.num_players)]\n",
    "        return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(surrogate, loss_fn, data_loader):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Setup.\n",
    "        device = next(surrogate.surrogate.parameters()).device\n",
    "        mean_loss = 0\n",
    "        N = 0\n",
    "        link=nn.Softmax(dim=1)\n",
    "\n",
    "        for x, y, S in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            S = S.to(device)\n",
    "            pred = surrogate(x, S)\n",
    "            loss = loss_fn(pred, y)\n",
    "            N += len(x)\n",
    "            mean_loss += len(x) * (loss - mean_loss) / N\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def generate_labels(dataset, model, batch_size):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Setup.\n",
    "        preds = []\n",
    "        if isinstance(model, torch.nn.Module):\n",
    "            device = next(model.parameters()).device\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "        for (x,) in loader:\n",
    "            pred = model(x.to(device)).cpu()\n",
    "            # print(\"Generate Labels\",pred.shape)\n",
    "            preds.append(pred)\n",
    "\n",
    "    return torch.cat(preds)\n",
    "\n",
    "\n",
    "class Surrogate:\n",
    "\n",
    "    def __init__(self, surrogate, num_features, groups=None):\n",
    "        # Store surrogate model.\n",
    "        self.surrogate = surrogate\n",
    "\n",
    "        # Store feature groups.\n",
    "        if groups is None:\n",
    "            self.num_players = num_features\n",
    "            self.groups_matrix = None\n",
    "        else:\n",
    "            # Verify groups.\n",
    "            inds_list = []\n",
    "            for group in groups:\n",
    "                inds_list += list(group)\n",
    "            assert np.all(np.sort(inds_list) == np.arange(num_features))\n",
    "\n",
    "            # Map groups to features.\n",
    "            self.num_players = len(groups)\n",
    "            device = next(surrogate.parameters()).device\n",
    "            self.groups_matrix = torch.zeros(\n",
    "                len(groups), num_features, dtype=torch.float32, device=device)\n",
    "            for i, group in enumerate(groups):\n",
    "                self.groups_matrix[i, group] = 1\n",
    "\n",
    "    def train_original_model(self,\n",
    "                             train_data,\n",
    "                             val_data,\n",
    "                             original_model,\n",
    "                             batch_size,\n",
    "                             max_epochs,\n",
    "                             loss_fn,\n",
    "                             validation_samples=1,\n",
    "                             validation_batch_size=None,\n",
    "                             lr=1e-3,\n",
    "                             min_lr=1e-5,\n",
    "                             lr_factor=0.5,\n",
    "                             weight_decay=0.01,\n",
    "                             lookback=5,\n",
    "                             training_seed=None,\n",
    "                             validation_seed=None,\n",
    "                             bar=False,\n",
    "                             verbose=False):\n",
    "\n",
    "        # Set up train dataset.\n",
    "        if isinstance(train_data, np.ndarray):\n",
    "            train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "\n",
    "        if isinstance(train_data, torch.Tensor):\n",
    "            train_set = TensorDataset(train_data)\n",
    "        elif isinstance(train_data, Dataset):\n",
    "            train_set = train_data\n",
    "        else:\n",
    "            raise ValueError('train_data must be either tensor or a '\n",
    "                             'PyTorch Dataset')\n",
    "\n",
    "        # Set up train data loader.\n",
    "        random_sampler = RandomSampler(\n",
    "            train_set, replacement=True,\n",
    "            num_samples=int(np.ceil(len(train_set) / batch_size))*batch_size)\n",
    "        batch_sampler = BatchSampler(\n",
    "            random_sampler, batch_size=batch_size, drop_last=True)\n",
    "        train_loader = DataLoader(train_set, batch_sampler=batch_sampler)\n",
    "\n",
    "        # Set up validation dataset.\n",
    "        sampler = UniformSampler(self.num_players)\n",
    "        if validation_seed is not None:\n",
    "            torch.manual_seed(validation_seed)\n",
    "        S_val = sampler.sample(len(val_data) * validation_samples)\n",
    "        if validation_batch_size is None:\n",
    "            validation_batch_size = batch_size\n",
    "\n",
    "        if isinstance(val_data, np.ndarray):\n",
    "            val_data = torch.tensor(val_data, dtype=torch.float32)\n",
    "\n",
    "        if isinstance(val_data, torch.Tensor):\n",
    "            # Generate validation labels.\n",
    "            y_val = generate_labels(TensorDataset(val_data), original_model,\n",
    "                                    validation_batch_size)\n",
    "            y_val_repeat = y_val.repeat(\n",
    "                validation_samples, *[1 for _ in y_val.shape[1:]])\n",
    "\n",
    "            # Create dataset.\n",
    "            val_data_repeat = val_data.repeat(validation_samples, 1)\n",
    "            # print(val_data_repeat.shape)\n",
    "            # print(y_val_repeat.shape)\n",
    "            # print(S_val.shape)\n",
    "            val_set = TensorDataset(val_data_repeat, y_val_repeat, S_val)\n",
    "        elif isinstance(val_data, Dataset):\n",
    "            # Generate validation labels.\n",
    "            y_val = generate_labels(val_data, original_model,\n",
    "                                    validation_batch_size)\n",
    "            y_val_repeat = y_val.repeat(\n",
    "                validation_samples, *[1 for _ in y_val.shape[1:]])\n",
    "\n",
    "            # Create dataset.\n",
    "            val_set = DatasetRepeat(\n",
    "                [val_data, TensorDataset(y_val_repeat, S_val)])\n",
    "        else:\n",
    "            raise ValueError('val_data must be either tuple of tensors or a '\n",
    "                             'PyTorch Dataset')\n",
    "\n",
    "        val_loader = DataLoader(val_set, batch_size=validation_batch_size)\n",
    "\n",
    "        # Setup for training.\n",
    "        surrogate = self.surrogate\n",
    "        device = next(surrogate.parameters()).device\n",
    "        optimizer = optim.AdamW(surrogate.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=lr_factor, patience=lookback // 2, min_lr=min_lr,verbose=verbose)\n",
    "        best_loss = 100000000\n",
    "        best_epoch = 0\n",
    "        best_model = deepcopy(surrogate)\n",
    "        loss_list = [best_loss]\n",
    "        if training_seed is not None:\n",
    "            torch.manual_seed(training_seed)\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            # Batch iterable.\n",
    "            if bar:\n",
    "                batch_iter = tqdm(train_loader, desc='Training epoch')\n",
    "            else:\n",
    "                batch_iter = train_loader\n",
    "\n",
    "            for (x,) in batch_iter:\n",
    "                # Prepare data.\n",
    "                x = x.to(device)\n",
    "\n",
    "                # Get original model prediction.\n",
    "                with torch.no_grad():\n",
    "                    y = original_model(x)\n",
    "\n",
    "                # Generate subsets.\n",
    "                S = sampler.sample(batch_size).to(device=device)\n",
    "                \n",
    "                # Make predictions.\n",
    "                pred = self.__call__(x, S)\n",
    "                #print(x.shape)\n",
    "                #print(pred.shape)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                # Optimizer step.\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                surrogate.zero_grad()\n",
    "\n",
    "            # Evaluate validation loss.\n",
    "            self.surrogate.eval()\n",
    "            val_loss = validate(self, loss_fn, val_loader).item()\n",
    "            self.surrogate.train()\n",
    "\n",
    "            # Print progress.\n",
    "            if verbose:\n",
    "                print('----- Epoch = {} -----'.format(epoch + 1))\n",
    "                print('Val loss = {:.4f}'.format(val_loss))\n",
    "                print('')\n",
    "            scheduler.step(val_loss)\n",
    "            loss_list.append(val_loss)\n",
    "\n",
    "            # Check if best model.\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model = deepcopy(surrogate)\n",
    "                best_epoch = epoch\n",
    "                if verbose:\n",
    "                    print('New best epoch, loss = {:.4f}'.format(val_loss))\n",
    "                    print('')\n",
    "            elif epoch - best_epoch == lookback:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "        # Clean up.\n",
    "        for param, best_param in zip(surrogate.parameters(), best_model.parameters()):\n",
    "            param.data = best_param.data\n",
    "        self.loss_list = loss_list\n",
    "        self.surrogate.eval()\n",
    "\n",
    "    def __call__(self, x, S):\n",
    "\n",
    "        if self.groups_matrix is not None:\n",
    "            S = torch.mm(S, self.groups_matrix)\n",
    "\n",
    "        return self.surrogate((x, S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastSHAP - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additive_efficient_normalization(pred, grand, null):\n",
    "\n",
    "    gap = (grand - null) - torch.sum(pred, dim=1)\n",
    "    # gap = gap.detach()\n",
    "    return pred + gap.unsqueeze(1) / pred.shape[1]\n",
    "\n",
    "\n",
    "def multiplicative_efficient_normalization(pred, grand, null):\n",
    "\n",
    "    ratio = (grand - null) / torch.sum(pred, dim=1)\n",
    "    # ratio = ratio.detach()\n",
    "    return pred * ratio.unsqueeze(1)\n",
    "\n",
    "\n",
    "def evaluate_explainer(explainer, normalization, x, grand, null, num_players, inference=False):\n",
    "\n",
    "    # Evaluate explainer.\n",
    "    # S=torch.ones_like(x)\n",
    "    # pred = explainer((x,S))\n",
    "    pred = explainer(x)\n",
    "\n",
    "    # Reshape SHAP values.\n",
    "    if len(pred.shape) == 4:\n",
    "        # Image.\n",
    "        image_shape = pred.shape\n",
    "        pred = pred.reshape(len(x), -1, num_players)\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "    else:\n",
    "        # Tabular.\n",
    "        image_shape = None\n",
    "        pred = pred.reshape(len(x), num_players, -1)\n",
    "\n",
    "    # For pre-normalization efficiency gap.\n",
    "    total = pred.sum(dim=1)\n",
    "\n",
    "    # Apply normalization.\n",
    "    if normalization:\n",
    "        pred = normalization(pred, grand, null)\n",
    "\n",
    "    # Reshape for inference.\n",
    "    if inference:\n",
    "        if image_shape is not None:\n",
    "            pred = pred.permute(0, 2, 1)\n",
    "            pred = pred.reshape(image_shape)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    return pred, total\n",
    "\n",
    "\n",
    "def calculate_grand_coalition(dataset, imputer, batch_size, link, device, num_workers):\n",
    "\n",
    "    ones = torch.ones(batch_size, imputer.num_players, dtype=torch.float32, device=device)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
    "                        pin_memory=True, num_workers=num_workers)\n",
    "    with torch.no_grad():\n",
    "        grand = []\n",
    "        for (x,) in loader:\n",
    "            grand.append(link(imputer(x.to(device), ones[:len(x)])))\n",
    "\n",
    "        # Concatenate and return.\n",
    "        grand = torch.cat(grand)\n",
    "        if len(grand.shape) == 1:\n",
    "            grand = grand.unsqueeze(1)\n",
    "\n",
    "    return grand\n",
    "\n",
    "\n",
    "def generate_validation_data(val_set, imputer, validation_samples, sampler, batch_size, link, device, num_workers):\n",
    "    \n",
    "    # Generate coalitions.\n",
    "    val_S = sampler.sample(\n",
    "        validation_samples * len(val_set), paired_sampling=True).reshape(\n",
    "        len(val_set), validation_samples, imputer.num_players)\n",
    "\n",
    "    # Get values.\n",
    "    val_values = []\n",
    "    for i in range(validation_samples):\n",
    "        # Set up data loader.\n",
    "        dset = DatasetRepeat([val_set, TensorDataset(val_S[:, i])])\n",
    "        loader = DataLoader(dset, batch_size=batch_size, shuffle=False,\n",
    "                            pin_memory=True, num_workers=num_workers)\n",
    "        values = []\n",
    "\n",
    "        for x, S in loader:\n",
    "            values.append(link(imputer(x.to(device), S.to(device))).cpu().data)\n",
    "\n",
    "        val_values.append(torch.cat(values))\n",
    "\n",
    "    val_values = torch.stack(val_values, dim=1)\n",
    "    return val_S, val_values\n",
    "\n",
    "\n",
    "def validate_explainer(val_loader, imputer, explainer, null, link, normalization):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Setup.\n",
    "        device = next(explainer.parameters()).device\n",
    "        mean_loss = 0\n",
    "        N = 0\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        for x, grand, S, values in val_loader:\n",
    "            # Move to device.\n",
    "            x = x.to(device)\n",
    "            S = S.to(device)\n",
    "            grand = grand.to(device)\n",
    "            values = values.to(device)\n",
    "\n",
    "            # Evaluate explainer.\n",
    "            pred, _ = evaluate_explainer(\n",
    "                explainer, normalization, x, grand, null, imputer.num_players)\n",
    "\n",
    "            # Calculate loss.\n",
    "            approx = null + torch.matmul(S, pred)\n",
    "            loss = loss_fn(approx, values)\n",
    "\n",
    "            # Update average.\n",
    "            N += len(x)\n",
    "            mean_loss += len(x) * (loss - mean_loss) / N\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "class FastSHAP:\n",
    "\n",
    "    def __init__(self,\n",
    "                 explainer,\n",
    "                 imputer,\n",
    "                 normalization='none',\n",
    "                 link=None):\n",
    "        # Set up explainer, imputer and link function.\n",
    "        self.explainer = explainer\n",
    "        self.imputer = imputer\n",
    "        self.num_players = imputer.num_players\n",
    "        self.null = None\n",
    "        if link is None or link == 'none':\n",
    "            self.link = nn.Identity()\n",
    "        elif isinstance(link, nn.Module):\n",
    "            self.link = link\n",
    "        else:\n",
    "            raise ValueError('unsupported link function: {}'.format(link))\n",
    "\n",
    "        # Set up normalization.\n",
    "        if normalization is None or normalization == 'none':\n",
    "            self.normalization = None\n",
    "        elif normalization == 'additive':\n",
    "            self.normalization = additive_efficient_normalization\n",
    "        elif normalization == 'multiplicative':\n",
    "            self.normalization = multiplicative_efficient_normalization\n",
    "        else:\n",
    "            raise ValueError('unsupported normalization: {}'.format(\n",
    "                normalization))\n",
    "\n",
    "    def train(self,\n",
    "              train_data,\n",
    "              val_data,\n",
    "              batch_size,\n",
    "              num_samples,\n",
    "              max_epochs,\n",
    "              lr=2e-4,\n",
    "              min_lr=1e-5,\n",
    "              lr_factor=0.5,\n",
    "              weight_decay=0.01,\n",
    "              eff_lambda=0,\n",
    "              paired_sampling=True,\n",
    "              validation_samples=None,\n",
    "              lookback=5,\n",
    "              training_seed=None,\n",
    "              validation_seed=None,\n",
    "              num_workers=0,\n",
    "              bar=False,\n",
    "              verbose=False):\n",
    "\n",
    "        # Set up explainer model.\n",
    "        explainer = self.explainer\n",
    "        num_players = self.num_players\n",
    "        imputer = self.imputer\n",
    "        link = self.link\n",
    "        normalization = self.normalization\n",
    "        explainer.train()\n",
    "        device = next(explainer.parameters()).device\n",
    "\n",
    "        # Verify other arguments.\n",
    "        if validation_samples is None:\n",
    "            validation_samples = num_samples\n",
    "\n",
    "        # Set up train dataset.\n",
    "        if isinstance(train_data, np.ndarray):\n",
    "            x_train = torch.tensor(train_data, dtype=torch.float32)\n",
    "            train_set = TensorDataset(x_train)\n",
    "        elif isinstance(train_data, torch.Tensor):\n",
    "            train_set = TensorDataset(train_data)\n",
    "        elif isinstance(train_data, Dataset):\n",
    "            train_set = train_data\n",
    "        else:\n",
    "            raise ValueError('train_data must be np.ndarray, torch.Tensor or Dataset')\n",
    "\n",
    "        # Set up validation dataset.\n",
    "        if isinstance(val_data, np.ndarray):\n",
    "            x_val = torch.tensor(val_data, dtype=torch.float32)\n",
    "            val_set = TensorDataset(x_val)\n",
    "        elif isinstance(val_data, torch.Tensor):\n",
    "            val_set = TensorDataset(val_data)\n",
    "        elif isinstance(val_data, Dataset):\n",
    "            val_set = val_data\n",
    "        else:\n",
    "            raise ValueError('train_data must be np.ndarray, torch.Tensor or Dataset')\n",
    "\n",
    "        # Grand coalition value.\n",
    "        grand_train = calculate_grand_coalition(\n",
    "            train_set, imputer, batch_size * num_samples, link, device,\n",
    "            num_workers).cpu()\n",
    "        grand_val = calculate_grand_coalition(\n",
    "            val_set, imputer, batch_size * num_samples, link, device,\n",
    "            num_workers).cpu()\n",
    "        \n",
    "        # print(\"grand_trian\",grand_train.shape)\n",
    "\n",
    "        # Null coalition.\n",
    "        with torch.no_grad():\n",
    "            zeros = torch.zeros(1, num_players, dtype=torch.float32, device=device)\n",
    "            null = link(imputer(train_set[0][0].unsqueeze(0).to(device), zeros))\n",
    "            if len(null.shape) == 1:\n",
    "                null = null.reshape(1, 1)\n",
    "        self.null = null\n",
    "\n",
    "        # Set up train loader.\n",
    "        # print(\"train_set\",len(train_set))\n",
    "        train_set = DatasetRepeat([train_set, TensorDataset(grand_train)])\n",
    "        # print(\"train_set_rep\",len(train_set))\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True, num_workers=num_workers)\n",
    "\n",
    "        # Generate validation data.\n",
    "        sampler = ShapleySampler(num_players)\n",
    "        if validation_seed is not None:\n",
    "            torch.manual_seed(validation_seed)\n",
    "        val_S, val_values = generate_validation_data(val_set, imputer, validation_samples, sampler, batch_size * num_samples, link, device, num_workers)\n",
    "\n",
    "        # Set up val loader.\n",
    "        val_set = DatasetRepeat(\n",
    "            [val_set, TensorDataset(grand_val, val_S, val_values)])\n",
    "        val_loader = DataLoader(val_set, batch_size=batch_size * num_samples, pin_memory=True, num_workers=num_workers)\n",
    "\n",
    "        # Setup for training.\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(explainer.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=lr_factor, patience=lookback // 2, min_lr=min_lr, verbose=verbose)\n",
    "        self.loss_list = []\n",
    "        best_loss = np.inf\n",
    "        best_epoch = -1\n",
    "        best_model = None\n",
    "        if training_seed is not None:\n",
    "            torch.manual_seed(training_seed)\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            # Batch iterable.\n",
    "            if bar:\n",
    "                batch_iter = tqdm(train_loader, desc='Training epoch')\n",
    "            else:\n",
    "                batch_iter = train_loader\n",
    "\n",
    "            for x, grand in batch_iter:\n",
    "                # Sample S.\n",
    "                S = sampler.sample(batch_size * num_samples, paired_sampling=paired_sampling)\n",
    "\n",
    "                # Move to device.\n",
    "                x = x.to(device)\n",
    "                S = S.to(device)\n",
    "                grand = grand.to(device)\n",
    "\n",
    "                # print(\"x\",x.shape, x)\n",
    "                # print(\"S\",S.shape, S)\n",
    "                # print(\"grand\",grand.shape, grand)\n",
    "\n",
    "                # Evaluate value function.\n",
    "                x_tiled = x.unsqueeze(1).repeat(\n",
    "                    1, num_samples, *[1 for _ in range(len(x.shape) - 1)]\n",
    "                    ).reshape(batch_size * num_samples, *x.shape[1:])\n",
    "                # print(\"x_tiled\",x_tiled.shape, x_tiled)\n",
    "                with torch.no_grad():\n",
    "                    values = link(imputer(x_tiled, S))\n",
    "                    # print(\"values\",values.shape, values)\n",
    "\n",
    "                # Evaluate explainer.\n",
    "                pred, total = evaluate_explainer(explainer, normalization, x, grand, null, num_players)\n",
    "                # print(\"pred\",pred.shape, pred)\n",
    "\n",
    "                # Calculate loss.\n",
    "                S = S.reshape(batch_size, num_samples, num_players)\n",
    "                # print(\"S\",S.shape, S)\n",
    "                values = values.reshape(batch_size, num_samples, -1)\n",
    "                # print(\"values\",values.shape, values)\n",
    "                # print(\"null\",null.shape, null)\n",
    "                # print(\"matmul\",torch.matmul(S, pred).shape, torch.matmul(S, pred))\n",
    "                approx = null + torch.matmul(S, pred)\n",
    "                # print(\"approx\",approx.shape, approx)\n",
    "                loss = loss_fn(approx, values)\n",
    "                if eff_lambda:\n",
    "                    loss = loss + eff_lambda * loss_fn(total, grand - null)\n",
    "\n",
    "                # Take gradient step.\n",
    "                loss = loss * num_players\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                explainer.zero_grad()\n",
    "\n",
    "            #     break\n",
    "            # break\n",
    "\n",
    "            # Evaluate validation loss.\n",
    "            explainer.eval()\n",
    "            val_loss = num_players * validate_explainer(val_loader, imputer, explainer, null, link, normalization).item()\n",
    "            explainer.train()\n",
    "\n",
    "            # Save loss, print progress.\n",
    "            if verbose:\n",
    "                print('----- Epoch = {} -----'.format(epoch + 1))\n",
    "                print(f'Val loss = {round(val_loss,6)}')\n",
    "                print('')\n",
    "            scheduler.step(val_loss)\n",
    "            self.loss_list.append(val_loss)\n",
    "\n",
    "            # Check for convergence.\n",
    "            if self.loss_list[-1] < best_loss:\n",
    "                best_loss = self.loss_list[-1]\n",
    "                best_epoch = epoch\n",
    "                best_model = deepcopy(explainer)\n",
    "                if verbose:\n",
    "                    print(f'New best epoch, loss = {round(val_loss,6)}')\n",
    "                    print('')\n",
    "            elif epoch - best_epoch == lookback:\n",
    "                if verbose:\n",
    "                    print('Stopping early at epoch = {}'.format(epoch))\n",
    "                break\n",
    "\n",
    "        # Copy best model.\n",
    "        for param, best_param in zip(explainer.parameters(), best_model.parameters()):\n",
    "            param.data = best_param.data\n",
    "        explainer.eval()\n",
    "\n",
    "    def shap_values(self, x):\n",
    "\n",
    "        # Data conversion.\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('data must be np.ndarray or torch.Tensor')\n",
    "\n",
    "        # Ensure null coalition is calculated.\n",
    "        device = next(self.explainer.parameters()).device\n",
    "        if self.null is None:\n",
    "            with torch.no_grad():\n",
    "                zeros = torch.zeros(1, self.num_players, dtype=torch.float32, device=device)\n",
    "                null = self.link(self.imputer(x[:1].to(device), zeros))\n",
    "            if len(null.shape) == 1:\n",
    "                null = null.reshape(1, 1)\n",
    "            self.null = null\n",
    "\n",
    "        # Generate explanations.\n",
    "        with torch.no_grad():\n",
    "            # Calculate grand coalition (for normalization).\n",
    "            if self.normalization:\n",
    "                grand = calculate_grand_coalition(\n",
    "                    x, self.imputer, len(x), self.link, device, 0)\n",
    "            else:\n",
    "                grand = None\n",
    "\n",
    "            # Evaluate explainer.\n",
    "            x = x.to(device)\n",
    "            pred = evaluate_explainer(\n",
    "                self.explainer, self.normalization, x, grand, self.null,\n",
    "                self.imputer.num_players, inference=True)\n",
    "\n",
    "        return pred.cpu().data.numpy()\n",
    "    \n",
    "    # def __call__(self, x, S):\n",
    "\n",
    "    #     return self.expl((x, S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "dataset=\"Census\"\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    *shap.datasets.adult(), test_size=0.2, random_state=7)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# Data scaling\n",
    "num_features = X_train.shape[1]\n",
    "feature_names = X_train.columns.tolist()\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train.values)\n",
    "X_val_s = ss.transform(X_val.values)\n",
    "X_test_s = ss.transform(X_test.values)\n",
    "\n",
    "print(*shap.datasets.adult()[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/magic04.data', dtype=object, delimiter=',')\n",
    "X = data[:,:-1]\n",
    "Y =data[:,-1]\n",
    "X = X.astype(float)\n",
    "Y = Y.astype(str)\n",
    "mapper={\"h\": 0, \"g\": 1}\n",
    "Y =[mapper[el] for el in Y]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=7)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# Data scaling\n",
    "num_features = X_train.shape[1]\n",
    "print(num_features)\n",
    "feature_names = [\"fLength\",\"fWidth\",\"fSize\",\"fConc\",\"fConc1\",\"fAsym\",\"fM3Long\", \"fM3Trans\", \"fAlpha\", \"fDist\"]\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_val_s = ss.transform(X_val)\n",
    "X_test_s = ss.transform(X_test)\n",
    "dataset=\"Magic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "dataset = pd.read_csv('data/credit_card.csv', sep=\",\")\n",
    "dataset=dataset.drop(\"ID\", axis=1)\n",
    "dataset.head()\n",
    "\n",
    "#mapper={\"present\":1,\"absent\":0}\n",
    "Y=dataset[\"DEFAULT_PAYMENT\"].values\n",
    "#Y=[mapper[el] for el in y]\n",
    "X=dataset.drop(\"DEFAULT_PAYMENT\",axis=1)\n",
    "columns=X.columns\n",
    "X=X.values\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=7)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "feature_names = columns\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_val_s = ss.transform(X_val)\n",
    "X_test_s = ss.transform(X_test)\n",
    "\n",
    "print(collections.Counter(Y_train))\n",
    "print(collections.Counter(Y_val))\n",
    "print(collections.Counter(Y_test))\n",
    "\n",
    "dataset=\"Credit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jannis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/jannis.txt', sep=\",\")\n",
    "columns=[f\"V{i}\" for i in range(0,dataset.shape[1]-1)]\n",
    "columns.append(\"Class\")\n",
    "dataset.columns=columns\n",
    "\n",
    "Y=dataset[\"Class\"].values\n",
    "#print(collections.Counter(Y))\n",
    "X=dataset.drop(\"Class\",axis=1)\n",
    "X=X.values\n",
    "#print(len(X),len(Y))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=7)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "feature_names = columns\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_val_s = ss.transform(X_val)\n",
    "X_test_s = ss.transform(X_test)\n",
    "\n",
    "print(num_features)\n",
    "print(collections.Counter(Y_train))\n",
    "print(collections.Counter(Y_val))\n",
    "print(collections.Counter(Y_test))\n",
    "\n",
    "dataset=\"Jannis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/data_banknote_authentication.txt', dtype=object, delimiter=',')\n",
    "X = data[:,:-1]\n",
    "Y =data[:,-1]\n",
    "Y = [int(el) for el in Y]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=7)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Data scaling\n",
    "num_features = X_train.shape[1]\n",
    "print(num_features)\n",
    "feature_names = [\"variance\",\"skewness\",\"curtosis\",\"entropy\"]\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_val_s = ss.transform(X_val)\n",
    "X_test_s = ss.transform(X_test)\n",
    "dataset=\"Bank\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/diabetes_data.txt', dtype=object, delimiter=',')\n",
    "X = data[:,:-1]\n",
    "Y =data[:,-1]\n",
    "mapper={\"tested_negative\": 0, \"tested_positive\": 1}\n",
    "Y =[mapper[el] for el in Y]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=7)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# Data scaling\n",
    "num_features = X_train.shape[1]\n",
    "print(num_features)\n",
    "feature_names = [\"preg\",\"plas\",\"pres\",\"skin\",\"insu\",\"mass\",\"pedi\", \"age\"]\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_val_s = ss.transform(X_val)\n",
    "X_test_s = ss.transform(X_test)\n",
    "dataset=\"Diabetes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mozilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/mozzilla_data.txt', dtype=object, delimiter=',')\n",
    "X = data[:,1:-1]\n",
    "Y =data[:,-1]\n",
    "Y=[int(el) for el in Y]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=7)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# Data scaling\n",
    "num_features = X_train.shape[1]\n",
    "print(num_features)\n",
    "feature_names = [\"start\",\"end\",\"event\",\"size\"]\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_val_s = ss.transform(X_val)\n",
    "X_test_s = ss.transform(X_test)\n",
    "dataset=\"Mozilla\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/phoneme.txt', dtype=object, delimiter=',')\n",
    "X = data[:,:-1]\n",
    "Y =data[:,-1]\n",
    "mapper={'1': 0, '2': 1}\n",
    "Y =[mapper[el] for el in Y]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=7)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# Data scaling\n",
    "num_features = X_train.shape[1]\n",
    "print(num_features)\n",
    "feature_names = [\"v1\",\"v2\",\"v3\",\"v4\",\"v5\"]\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_val_s = ss.transform(X_val)\n",
    "X_test_s = ss.transform(X_test)\n",
    "dataset=\"Phoneme\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/click.arff', sep=\",\", header=None)\n",
    "dataset.shape\n",
    "print(dataset[0].value_counts()/len(dataset))\n",
    "data=dataset.values\n",
    "X=data[:,1:]\n",
    "Y=data[:,0]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=7)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# Data scaling\n",
    "num_features = X_train.shape[1]\n",
    "print(num_features)\n",
    "# feature_names = ['Area', 'Perimeter', 'Major_Axis', 'Minor_Axis', 'Eccentricity', 'Convex_Area', 'Extent_Real']\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_val_s = ss.transform(X_val)\n",
    "X_test_s = ss.transform(X_test)\n",
    "dataset=\"Click\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bankruptcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/bank.csv', sep=\";\")\n",
    "dataset.head()\n",
    "X=dataset.drop(\"y\",axis=1)\n",
    "columns=X.columns\n",
    "\n",
    "Y=dataset[\"y\"]\n",
    "mapper={\"no\":0,\"yes\":1}\n",
    "Y=[mapper[el] for el in Y]\n",
    "\n",
    "# Encode categorical variables on X with EncoderLabel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "for col in ['job', 'marital', 'education', 'default', 'housing',\n",
    "       'loan', 'contact', 'month', 'poutcome']:\n",
    "    X[col] = encoder.fit_transform(X[col])\n",
    "\n",
    "X=X.values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=7)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# Data scaling\n",
    "num_features = X_train.shape[1]\n",
    "print(num_features)\n",
    "feature_names = columns\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_val_s = ss.transform(X_val)\n",
    "X_test_s = ss.transform(X_test)\n",
    "dataset=\"Bankruptcy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDD Cup 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=\"\"\"duration,\n",
    "protocol_type,\n",
    "service,\n",
    "flag,\n",
    "src_bytes,\n",
    "dst_bytes,\n",
    "land,\n",
    "wrong_fragment,\n",
    "urgent,\n",
    "hot,\n",
    "num_failed_logins,\n",
    "logged_in,\n",
    "num_compromised,\n",
    "root_shell,\n",
    "su_attempted,\n",
    "num_root,\n",
    "num_file_creations,\n",
    "num_shells,\n",
    "num_access_files,\n",
    "num_outbound_cmds,\n",
    "is_host_login,\n",
    "is_guest_login,\n",
    "count,\n",
    "srv_count,\n",
    "serror_rate,\n",
    "srv_serror_rate,\n",
    "rerror_rate,\n",
    "srv_rerror_rate,\n",
    "same_srv_rate,\n",
    "diff_srv_rate,\n",
    "srv_diff_host_rate,\n",
    "dst_host_count,\n",
    "dst_host_srv_count,\n",
    "dst_host_same_srv_rate,\n",
    "dst_host_diff_srv_rate,\n",
    "dst_host_same_src_port_rate,\n",
    "dst_host_srv_diff_host_rate,\n",
    "dst_host_serror_rate,\n",
    "dst_host_srv_serror_rate,\n",
    "dst_host_rerror_rate,\n",
    "dst_host_srv_rerror_rate\"\"\"\n",
    "\n",
    "columns=[]\n",
    "for c in cols.split(','):\n",
    "    if(c.strip()):\n",
    "       columns.append(c.strip())\n",
    "\n",
    "columns.append('target')\n",
    "#print(columns)\n",
    "print(len(columns))\n",
    "\n",
    "attacks_types = {\n",
    "    'normal': 'normal',\n",
    "'back': 'dos',\n",
    "'buffer_overflow': 'u2r',\n",
    "'ftp_write': 'r2l',\n",
    "'guess_passwd': 'r2l',\n",
    "'imap': 'r2l',\n",
    "'ipsweep': 'probe',\n",
    "'land': 'dos',\n",
    "'loadmodule': 'u2r',\n",
    "'multihop': 'r2l',\n",
    "'neptune': 'dos',\n",
    "'nmap': 'probe',\n",
    "'perl': 'u2r',\n",
    "'phf': 'r2l',\n",
    "'pod': 'dos',\n",
    "'portsweep': 'probe',\n",
    "'rootkit': 'u2r',\n",
    "'satan': 'probe',\n",
    "'smurf': 'dos',\n",
    "'spy': 'r2l',\n",
    "'teardrop': 'dos',\n",
    "'warezclient': 'r2l',\n",
    "'warezmaster': 'r2l',\n",
    "}\n",
    "\n",
    "path = \"data/KDDcup99/kddcup.data.corrected\"\n",
    "df = pd.read_csv(path,names=columns)\n",
    "\n",
    "df['Attack Type'] = df.target.apply(lambda r:attacks_types[r[:-1]])\n",
    "\n",
    "#Finding categorical features\n",
    "num_cols = df._get_numeric_data().columns\n",
    "\n",
    "cate_cols = list(set(df.columns)-set(num_cols))\n",
    "cate_cols.remove('target')\n",
    "cate_cols.remove('Attack Type')\n",
    "\n",
    "cate_cols\n",
    "\n",
    "df = df.dropna('columns')# drop columns with NaN\n",
    "\n",
    "df = df[[col for col in df if df[col].nunique() > 1]]# keep columns where there are more than 1 unique values\n",
    "\n",
    "df.drop('num_root',axis = 1,inplace = True)\n",
    "df.drop('srv_serror_rate',axis = 1,inplace = True)\n",
    "df.drop('srv_rerror_rate',axis = 1, inplace=True)\n",
    "df.drop('dst_host_srv_serror_rate',axis = 1, inplace=True)\n",
    "df.drop('dst_host_serror_rate',axis = 1, inplace=True)\n",
    "df.drop('dst_host_rerror_rate',axis = 1, inplace=True)\n",
    "df.drop('dst_host_srv_rerror_rate',axis = 1, inplace=True)\n",
    "df.drop('dst_host_same_srv_rate',axis = 1, inplace=True)\n",
    "\n",
    "pmap = {'icmp':0,'tcp':1,'udp':2}\n",
    "df['protocol_type'] = df['protocol_type'].map(pmap)\n",
    "\n",
    "fmap = {'SF':0,'S0':1,'REJ':2,'RSTR':3,'RSTO':4,'SH':5 ,'S1':6 ,'S2':7,'RSTOS0':8,'S3':9 ,'OTH':10}\n",
    "df['flag'] = df['flag'].map(fmap)\n",
    "\n",
    "df.drop('service',axis = 1,inplace= True)\n",
    "\n",
    "df = df.drop(['target',], axis=1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d1 = df[:77753]\n",
    "df_d2 = df[77753:155506]\n",
    "df_d3 = df[155506:233259]\n",
    "df_d4 = df[233259:311012]\n",
    "df_d5 = df[311012:388765]\n",
    "df_d6 = df[388765:466518]\n",
    "\n",
    "df_d=df_d1\n",
    "dataset= \"KDD_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map Attack Type to 0 if normal, 1 otherwise\n",
    "df_d['Label'] = df_d['Attack Type'].apply(lambda x: 0 if x == 'normal' else 1)\n",
    "\n",
    "Y = df_d[['Label']]\n",
    "X = df_d.drop(['Attack Type',], axis=1)\n",
    "feature_names = X.columns\n",
    "\n",
    "X=X.values\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "num_features = X_train.shape[1]\n",
    "print(X_train.shape, X_val.shape, num_features, feature_names)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_val_s = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/BitcoinHistoryDataset'\n",
    "data_path = os.path.join(data_dir, 'BTC-2019min.csv')\n",
    "data = pd.read_csv(data_path, engine='c')\n",
    "data.head()\n",
    "\n",
    "# split date into year, month, day, hour\n",
    "data['Date'] = pd.to_datetime(data['date'])\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "data['Hour'] = data['Date'].dt.hour\n",
    "data['Minute'] = data['Date'].dt.minute\n",
    "\n",
    "# sort by date\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# drop column unix, date, Date, symbol\n",
    "data = data.drop(['unix', 'date', 'symbol'], axis=1)\n",
    "\n",
    "old_columns = data.columns\n",
    "    \n",
    "def get_features(df):\n",
    "    df['price mean'] = df[['open', 'high', 'low', 'close']].mean(axis = 1)\n",
    "    df['upper shadow'] = df['high'] - np.maximum(df['open'], df['close'])\n",
    "    df['lower shadow'] = np.minimum(df['open'], df['close']) - df['low']\n",
    "    df['spread'] = df['high'] - df['low']\n",
    "    df['trade'] = df['close'] - df['open']\n",
    "    df['open close LPC'] = np.log(df['close'] / df['open'])\n",
    "    df['10 period SMA'] = df['close'].rolling(10).mean().fillna(0)\n",
    "    df['20 period SMA'] = df['close'].rolling(20).mean().fillna(0)\n",
    "    df['5 period LR'] = pd.Series(np.log(df['close'])).diff(periods=5).fillna(0)\n",
    "    df['10 period LR'] = pd.Series(np.log(df['close'])).diff(periods=10).fillna(0)\n",
    "    df['log norm close'] = np.log(df['close'] + 1)/10\n",
    "    df['buy/sell'] = df['close'].diff(periods=1)\n",
    "    df = df.copy().loc[df['buy/sell'].notna()]\n",
    "    df['buy/sell'] = df['buy/sell'].apply(lambda x: 0 if x<=0 else 1)\n",
    "    return df\n",
    "\n",
    "#compute features and drop irrelavant column\n",
    "data = get_features(data)\n",
    "data.head()\n",
    "\n",
    "# drop columns open, high, low, close\n",
    "data = data.drop(['open', 'high', 'low', 'close','Year'], axis=1)\n",
    "\n",
    "df_2019 = data[data['Year'] == 2019]\n",
    "\n",
    "df_01 = data[data['Month'] == 1]\n",
    "df_02 = data[data['Month'] == 2]\n",
    "df_03 = data[data['Month'] == 3]\n",
    "df_04 = data[data['Month'] == 4]\n",
    "df_05 = data[data['Month'] == 5]\n",
    "df_06 = data[data['Month'] == 6]\n",
    "df_07 = data[data['Month'] == 7]\n",
    "df_08 = data[data['Month'] == 8]\n",
    "df_09 = data[data['Month'] == 9]\n",
    "df_10 = data[data['Month'] == 10]\n",
    "df_11 = data[data['Month'] == 11]\n",
    "df_12 = data[data['Month'] == 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y=df_01\n",
    "dataset= \"BTC_2019m_01\"\n",
    "\n",
    "\n",
    "Y=df_y['buy/sell']\n",
    "tmp=df_y.drop(['buy/sell','Date'], axis=1)\n",
    "X=tmp.values\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "num_features = X_train.shape[1]\n",
    "feature_names = tmp.columns\n",
    "print(X_train.shape, X_val.shape, num_features, feature_names)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_val_s = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black-Box Model - Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_value)\n",
    "# tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "# tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "# for later versions:\n",
    "# session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "# tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbmodel = Sequential()\n",
    "bbmodel.add(Dense(64, activation='relu', input_shape=(num_features,)))\n",
    "bbmodel.add(Dropout(0.5))\n",
    "bbmodel.add(Dense(64, activation='relu'))\n",
    "bbmodel.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "bbmodel.add(Dense(2))\n",
    "bbmodel.add(Activation('softmax'))\n",
    "\n",
    "# Compile the model\n",
    "bbmodel.compile(optimizer='adam',\n",
    "            #   loss='binary_crossentropy',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "bbmodel.fit(X_train_s, Y_train, epochs=50, batch_size=32, validation_data=(X_val_s, Y_val), verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = bbmodel.evaluate(X_test_s, Y_test, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surrogate - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all random seeds\n",
    "SEED=42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Select device\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "def original_model(x):\n",
    "    pred = bbmodel.predict(x.cpu().numpy())\n",
    "    return torch.tensor(pred, dtype=torch.float32, device=x.device)\n",
    "\n",
    "\n",
    "# Check for model\n",
    "if os.path.isfile(f'checkpoints/{dataset}_surrogate.pt'): \n",
    "    print('Loading saved surrogate model')\n",
    "    surr = torch.load(f'checkpoints/{dataset}_surrogate.pt').to(device)\n",
    "    surrogate = Surrogate(surr, num_features)\n",
    "else:\n",
    "    if dataset==\"Census\": # SAME PARAMETERS AS IN FASTSHAP\n",
    "        print(\"Census!\")\n",
    "        surr = nn.Sequential(\n",
    "                MaskLayer1d(value=0, append=True),\n",
    "                nn.Linear(2 * num_features, 128),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.Linear(128, 2)).to(device)\n",
    "\n",
    "        # Set up surrogate object\n",
    "        surrogate = Surrogate(surr, num_features)\n",
    "\n",
    "        # Train\n",
    "        start=time.time()\n",
    "        surrogate.train_original_model(\n",
    "            X_train_s,\n",
    "            X_val_s[:200],\n",
    "            original_model,\n",
    "            batch_size=32,\n",
    "            max_epochs=200,\n",
    "            loss_fn=KLDivLoss(),  #KLDivLoss(),\n",
    "            lookback=10,\n",
    "            validation_samples=128,\n",
    "            validation_batch_size=10000,\n",
    "            training_seed=SEED,\n",
    "            verbose=True)\n",
    "        end=time.time()\n",
    "        print(\"Training Time:\",(end-start))\n",
    "    else:\n",
    "        print(dataset)\n",
    "        Layer_size=512\n",
    "        surr = nn.Sequential(\n",
    "            MaskLayer1d(value=0, append=True), \n",
    "            nn.Linear( 2*num_features, Layer_size), \n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(Layer_size, Layer_size),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(Layer_size, 2),\n",
    "        ).to(device)\n",
    "\n",
    "        surrogate = Surrogate(surr, num_features)\n",
    "\n",
    "        start=time.time()\n",
    "        surrogate.train_original_model(\n",
    "            X_train_s,\n",
    "            X_val_s,\n",
    "            original_model,\n",
    "            batch_size=32, #8\n",
    "            max_epochs=200,\n",
    "            loss_fn=KLDivLoss(),\n",
    "            validation_samples=10,\n",
    "            validation_batch_size=10000,\n",
    "            verbose=True,\n",
    "            lr=1e-4, #1e-4\n",
    "            min_lr=1e-8,\n",
    "            lr_factor=0.5,\n",
    "            weight_decay=0.01,\n",
    "            training_seed=SEED,\n",
    "            lookback=20\n",
    "        )\n",
    "        end=time.time()\n",
    "        print((end-start))\n",
    "    \n",
    "    surr.cpu()\n",
    "    torch.save(surr, f'checkpoints/{dataset}_surrogate.pt')\n",
    "    surr.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastSHAP - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "print(device)\n",
    "\n",
    "# Check for model\n",
    "if os.path.isfile(f'checkpoints/{dataset}_explainer.pt'): \n",
    "    print('Loading saved explainer model')\n",
    "    explainer = torch.load(f'checkpoints/{dataset}_explainer.pt').to(device)\n",
    "    fastshap = FastSHAP(explainer, surrogate, normalization='additive', link=nn.Softmax(dim=-1))\n",
    "else:\n",
    "    if dataset==\"Census\": # SAME PARAMETERS AS IN FASTSHAP\n",
    "        print(\"Census!\")\n",
    "        explainer = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2 * num_features)).to(device)\n",
    "\n",
    "        # Set up FastSHAP object\n",
    "        fastshap = FastSHAP(explainer, surrogate, normalization='additive', link=nn.Softmax(dim=-1))\n",
    "\n",
    "        # Train\n",
    "        start=time.time()\n",
    "        fastshap.train(\n",
    "            X_train_s,\n",
    "            X_val_s[:200],\n",
    "            batch_size=32,\n",
    "            num_samples=32,\n",
    "            max_epochs=200,\n",
    "            lookback=10,\n",
    "            validation_samples=128,\n",
    "            training_seed=SEED,\n",
    "            verbose=True)\n",
    "        end=time.time()\n",
    "        print(\"Training Time:\",(end-start))\n",
    "    else:\n",
    "        print(dataset)\n",
    "        LAYER_SIZE=256\n",
    "        explainer = nn.Sequential(\n",
    "            # MaskLayer1d(value=0, append=True),\n",
    "            nn.Linear(num_features, LAYER_SIZE),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(LAYER_SIZE, LAYER_SIZE),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(LAYER_SIZE, 2 * num_features)).to(device)\n",
    "\n",
    "        # Set up FastSHAP object\n",
    "        fastshap = FastSHAP(explainer, surrogate, normalization='additive', link=nn.Softmax(dim=-1))\n",
    "\n",
    "        # Train\\\n",
    "        start=time.time()\n",
    "        fastshap.train(\n",
    "            X_train_s,\n",
    "            X_val_s[:200],\n",
    "            batch_size=32,\n",
    "            num_samples=32,\n",
    "            max_epochs=400,#200\n",
    "            validation_samples=32, #128\n",
    "            verbose=True,\n",
    "            paired_sampling=True,\n",
    "            lr=2e-4, #1e-4\n",
    "            min_lr=1e-8,\n",
    "            lr_factor=0.5,\n",
    "            weight_decay=0.01,\n",
    "            training_seed=SEED,\n",
    "            lookback=20,\n",
    "        )\n",
    "        end=time.time()\n",
    "        print((end-start))\n",
    "\n",
    "    \n",
    "    explainer.cpu()\n",
    "    torch.save(explainer, f'checkpoints/{dataset}_explainer.pt')\n",
    "    explainer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightningSHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_STFS(model, loss_fn1, loss_fn2, data_loader, batch_size, num_samples, sampler, sampler_surr, paired_sampling, epoch, loss_used):\n",
    "    #print('validate_STFS')\n",
    "    with torch.no_grad():\n",
    "        # Setup.\n",
    "        device = next(model.model.parameters()).device\n",
    "        mean_loss = 0\n",
    "        mean_loss1 = 0\n",
    "        mean_loss2 = 0\n",
    "        mean_loss3 = 0\n",
    "        mean_loss4 = 0\n",
    "        mean_loss5 = 0\n",
    "        N = 0\n",
    "        link=nn.Softmax(dim=-1)\n",
    "\n",
    "        # COMPUTE NULL COALITION\n",
    "        sample=data_loader.dataset[0][0]\n",
    "        sample = sample.to(device)\n",
    "        zeros=torch.zeros(1, model.num_players, device=device)\n",
    "        null=model(sample, zeros)\n",
    "        null_reshape = null.reshape(1, model.num_players, -1)\n",
    "        null_sum = null_reshape.sum(dim=1)\n",
    "        null=link(null_sum)\n",
    "        if len(null.shape) == 1:\n",
    "            null = null.reshape(1, 1)\n",
    "\n",
    "        # print(\"VALIDATION\")\n",
    "\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # Generate subsets.\n",
    "            S = sampler.sample(batch_size*num_samples, paired_sampling=paired_sampling).to(device=device)\n",
    "            S_surr = sampler_surr.sample(batch_size).to(device=device)\n",
    "\n",
    "            pred_xs = model(x, S_surr)\n",
    "            pred_xs_reshape = pred_xs.reshape(len(x), model.num_players, -1)\n",
    "            pred_xs_sum = pred_xs_reshape.sum(dim=1)\n",
    "\n",
    "            loss1 = loss_fn1(pred_xs_sum, y)\n",
    "\n",
    "            ones=torch.ones_like(x).to(device)\n",
    "            pred=model(x, ones)\n",
    "            pred_reshape = pred.reshape(len(x), model.num_players, -1)\n",
    "            grand_sum = pred_reshape.sum(dim=1)\n",
    "            grand=link(grand_sum)\n",
    "            \n",
    "            pred_eff = additive_efficient_normalization(pred_reshape, y, null) ################### NORMALIZATION WITH Y\n",
    "            total=pred_eff.sum(dim=1)\n",
    "\n",
    "            x_tiled = x.unsqueeze(1).repeat(\n",
    "                1, num_samples, *[1 for _ in range(len(x.shape) - 1)]\n",
    "                ).reshape(batch_size * num_samples, *x.shape[1:])\n",
    "            \n",
    "            \n",
    "            val = model(x_tiled, S)\n",
    "            val_reshape = val.reshape(len(x_tiled), model.num_players, -1)\n",
    "            val_sum = val_reshape.sum(dim=1)\n",
    "\n",
    "            values = link(val_sum)\n",
    "            \n",
    "            S=S.reshape(batch_size, num_samples, model.num_players)\n",
    "            values=values.reshape(batch_size, num_samples, -1)\n",
    "\n",
    "            approx = null + torch.matmul(S, pred_eff)\n",
    "\n",
    "            loss2 = loss_fn2(approx, values)\n",
    "            loss2 = loss2 * 1\n",
    "            loss3 = loss_fn2(total, y-null)\n",
    "\n",
    "            ####################################################################################\n",
    "            ################### FORCE LOSS ON SINGLE FEATURE AT TIME ###########################\n",
    "            eye=torch.eye(model.num_players)\n",
    "            eye=eye.repeat(batch_size, 1).to(device)\n",
    "            x_extended=torch.cat([el.unsqueeze(0).repeat(model.num_players,1) for el in x])\n",
    "            single_pred=model(x_extended, eye)\n",
    "            single_pred=single_pred.reshape(batch_size*model.num_players, model.num_players, -1)\n",
    "            single_pred_sum = single_pred.sum(dim=1)\n",
    "            single_pred_surr=link(single_pred_sum)\n",
    "            single_pred_eff=additive_efficient_normalization(single_pred, single_pred_surr, null)\n",
    "            delta=single_pred_surr-null\n",
    "            tmp=[]\n",
    "            i=0\n",
    "            for el in single_pred_eff:\n",
    "                tmp.append(el[i].unsqueeze(0))\n",
    "                i+=1\n",
    "                if i==model.num_players:\n",
    "                    i=0\n",
    "            tmp=torch.cat(tmp)\n",
    "            loss4=loss_fn2(tmp, delta)\n",
    "            ####################################################################################\n",
    "\n",
    "            loss5=loss_fn2(y,grand)\n",
    "\n",
    "            if epoch>0:\n",
    "                loss = loss1 + loss2 #+ loss4 + loss5\n",
    "                if \"L3\" in loss_used:\n",
    "                    loss += loss3\n",
    "                if \"L4\" in loss_used:\n",
    "                    loss += loss4\n",
    "                if \"L5\" in loss_used:\n",
    "                    loss += loss5\n",
    "            else:\n",
    "                loss = loss1\n",
    "\n",
    "            N += len(x)\n",
    "            mean_loss += len(x) * (loss - mean_loss) / N\n",
    "            mean_loss1 += len(x) * (loss1 - mean_loss1) / N\n",
    "            mean_loss2 += len(x) * (loss2 - mean_loss2) / N\n",
    "            mean_loss3 += len(x) * (loss3 - mean_loss3) / N\n",
    "            mean_loss4 += len(x) * (loss4 - mean_loss4) / N\n",
    "            mean_loss5 += len(x) * (loss5 - mean_loss5) / N\n",
    "\n",
    "    return mean_loss, mean_loss1, mean_loss2, mean_loss3, mean_loss4, mean_loss5\n",
    "\n",
    "\n",
    "def generate_labels_STFS(dataset, model, batch_size):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Setup.\n",
    "        preds = []\n",
    "        if isinstance(model, torch.nn.Module):\n",
    "            device = next(model.parameters()).device\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "        for (x,) in loader:\n",
    "            pred = model(x.to(device)).cpu()\n",
    "            preds.append(pred)\n",
    "\n",
    "    return torch.cat(preds)\n",
    "\n",
    "def additive_efficient_normalization(pred, grand, null):\n",
    "    gap = (grand - null) - torch.sum(pred, dim=1)\n",
    "    return pred + gap.unsqueeze(1) / pred.shape[1]\n",
    "\n",
    "\n",
    "def multiplicative_efficient_normalization(pred, grand, null):\n",
    "    ratio = (grand - null) / torch.sum(pred, dim=1)\n",
    "    return pred * ratio.unsqueeze(1)\n",
    "\n",
    "\n",
    "class LightningSHAP:\n",
    "\n",
    "    def __init__(self, model, om, num_features, groups=None):\n",
    "        # Store surrogate model.\n",
    "        self.model = model\n",
    "        self.batch_size = None\n",
    "        self.validation_batch_size = None\n",
    "        self.num_samples = None\n",
    "        self.link = None\n",
    "        self.bbm=om\n",
    "\n",
    "        # Store feature groups.\n",
    "        if groups is None:\n",
    "            self.num_players = num_features\n",
    "            self.groups_matrix = None\n",
    "        else:\n",
    "            # Verify groups.\n",
    "            inds_list = []\n",
    "            for group in groups:\n",
    "                inds_list += list(group)\n",
    "            assert np.all(np.sort(inds_list) == np.arange(num_features))\n",
    "\n",
    "            # Map groups to features.\n",
    "            self.num_players = len(groups)\n",
    "            device = next(surrogate.parameters()).device\n",
    "            self.groups_matrix = torch.zeros(\n",
    "                len(groups), num_features, dtype=torch.float32, device=device)\n",
    "            for i, group in enumerate(groups):\n",
    "                self.groups_matrix[i, group] = 1\n",
    "\n",
    "                \n",
    "    def train_original_model(self,\n",
    "                             train_data,\n",
    "                             val_data,\n",
    "                             original_model,\n",
    "                             batch_size,\n",
    "                             max_epochs,\n",
    "                             loss_fn1,\n",
    "                             loss_fn2,\n",
    "                             validation_samples=1,\n",
    "                             validation_batch_size=None,\n",
    "                             lr=None,\n",
    "                             min_lr=None,\n",
    "                             lr_factor=None,\n",
    "                             weight_decay=None,\n",
    "                             lookback=None,\n",
    "                             num_samples=None,\n",
    "                             training_seed=None,\n",
    "                             validation_seed=None,\n",
    "                             paired_sampling=False,\n",
    "                             bar=False,\n",
    "                             verbose=False,\n",
    "                             loss_used=[\"L3\",\"L4\",\"L5\"]):\n",
    "\n",
    "        # Set up train dataset.\n",
    "        if isinstance(train_data, np.ndarray):\n",
    "            train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "        y_tr = generate_labels_STFS(TensorDataset(train_data), original_model, batch_size)\n",
    "\n",
    "        if isinstance(train_data, torch.Tensor):\n",
    "            train_set = TensorDataset(train_data, y_tr)\n",
    "        elif isinstance(train_data, Dataset):\n",
    "            train_set = train_data\n",
    "        else:\n",
    "            raise ValueError('train_data must be either tensor or a PyTorch Dataset')\n",
    "\n",
    "        # Set up train data loader.\n",
    "        random_sampler = RandomSampler(train_set, replacement=True, num_samples=int(np.ceil(len(train_set) / batch_size))*batch_size)\n",
    "        batch_sampler = BatchSampler(random_sampler, batch_size=batch_size, drop_last=True)\n",
    "        train_loader = DataLoader(train_set, batch_sampler=batch_sampler, num_workers=4)\n",
    "\n",
    "        # Set up validation dataset.\n",
    "        sampler_surr=UniformSampler(self.num_players)\n",
    "        sampler = ShapleySampler(self.num_players)\n",
    "        if validation_seed is not None:\n",
    "            torch.manual_seed(validation_seed)\n",
    "        # S_val = sampler.sample(len(val_data) * num_samples, paired_sampling=paired_sampling)\n",
    "        # S_val_surr = sampler_surr.sample(len(val_data))\n",
    "        if validation_batch_size is None:\n",
    "            validation_batch_size = batch_size\n",
    "\n",
    "        if isinstance(val_data, np.ndarray):\n",
    "            val_data = torch.tensor(val_data, dtype=torch.float32)\n",
    "\n",
    "        if isinstance(val_data, torch.Tensor):\n",
    "            # Generate validation labels.\n",
    "            y_val = generate_labels_STFS(TensorDataset(val_data), original_model, validation_batch_size)\n",
    "            y_val_repeat = y_val.repeat(validation_samples, *[1 for _ in y_val.shape[1:]])\n",
    "\n",
    "            # Create dataset.\n",
    "            val_data_repeat = val_data.repeat(validation_samples, 1)\n",
    "            val_set = TensorDataset(val_data_repeat, y_val_repeat)\n",
    "        else:\n",
    "            raise ValueError('val_data must be either tuple of tensors or a PyTorch Dataset')\n",
    "\n",
    "        val_loader = DataLoader(val_set, batch_size=validation_batch_size, drop_last=True, num_workers=4)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_batch_size = validation_batch_size\n",
    "        self.num_samples = num_samples \n",
    "        # self.bbm = original_model\n",
    "\n",
    "        # Setup for training.\n",
    "        link=nn.Softmax(dim=-1)\n",
    "        model = self.model\n",
    "        device = next(model.parameters()).device\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        #optimizer = MTAdam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=lr_factor, patience=int(lookback // 2), min_lr=min_lr,verbose=verbose)\n",
    "        best_loss = 100000\n",
    "        best_epoch = 0\n",
    "        best_model = deepcopy(model)\n",
    "        val_loss_list = []\n",
    "        val_loss1_list = []\n",
    "        val_loss2_list = []\n",
    "        val_loss3_list = []\n",
    "        val_loss4_list = []\n",
    "        val_loss5_list = []\n",
    "        train_loss_list = []\n",
    "        train_loss1_list = []\n",
    "        train_loss2_list = []\n",
    "        train_loss3_list = []\n",
    "        train_loss4_list = []\n",
    "        train_loss5_list = []\n",
    "        if training_seed is not None:\n",
    "            torch.manual_seed(training_seed)\n",
    "\n",
    "        #print('STFS_training')\n",
    "        print(\"#\"*50)\n",
    "        print('Training surrogate model with LOSSES:',loss_used)\n",
    "        print(\"#\"*50)\n",
    "        for epoch in range(max_epochs):\n",
    "            # Batch iterable.\n",
    "            if bar:\n",
    "                batch_iter = tqdm(train_loader, desc='Training epoch')\n",
    "            else:\n",
    "                batch_iter = train_loader\n",
    "\n",
    "            mean_loss = 0\n",
    "            mean_loss1 = 0\n",
    "            mean_loss2 = 0\n",
    "            mean_loss3 = 0\n",
    "            mean_loss4 = 0\n",
    "            mean_loss5 = 0\n",
    "            N = 0\n",
    "\n",
    "            iter=0\n",
    "            for (x,y) in batch_iter:\n",
    "                iter+=1\n",
    "                # Prepare data.\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # Generate subsets.\n",
    "                S = sampler.sample(batch_size*num_samples, paired_sampling=paired_sampling).to(device=device)\n",
    "                S_surr = sampler_surr.sample(batch_size).to(device=device)\n",
    "\n",
    "                pred_xs = self.__call__(x, S_surr)\n",
    "                pred_xs_reshape = pred_xs.reshape(len(x), self.num_players, -1)\n",
    "                pred_xs_sum = pred_xs_reshape.sum(dim=1)\n",
    "\n",
    "                loss1 = loss_fn1(pred_xs_sum, y)\n",
    "\n",
    "                # COMPUTE NULL COALITION\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    zeros=torch.zeros(1, self.num_players, device=device)\n",
    "                    null=self.__call__(x[:1], zeros)\n",
    "                    null_reshape = null.reshape(1, self.num_players, -1)\n",
    "                    null_sum = null_reshape.sum(dim=1)\n",
    "                    null=link(null_sum)\n",
    "                    if len(null.shape) == 1:\n",
    "                        null = null.reshape(1, 1)\n",
    "                self.model.train()\n",
    "\n",
    "                ones=torch.ones_like(x).to(device)\n",
    "                pred=self.__call__(x, ones)\n",
    "                pred_reshape = pred.reshape(len(x), self.num_players, -1)\n",
    "                grand_sum = pred_reshape.sum(dim=1)\n",
    "                grand=link(grand_sum)\n",
    "                \n",
    "                pred_eff = additive_efficient_normalization(pred_reshape, y, null)\n",
    "                total=pred_eff.sum(dim=1)\n",
    "\n",
    "\n",
    "                x_tiled = x.unsqueeze(1).repeat(\n",
    "                    1, num_samples, *[1 for _ in range(len(x.shape) - 1)]\n",
    "                    ).reshape(batch_size * num_samples, *x.shape[1:])\n",
    "                \n",
    "                val = self.__call__(x_tiled, S)\n",
    "                val_reshape = val.reshape(len(x_tiled), self.num_players, -1)\n",
    "                val_sum = val_reshape.sum(dim=1)\n",
    "\n",
    "                values = link(val_sum)\n",
    "                \n",
    "                S=S.reshape(batch_size, num_samples, self.num_players)\n",
    "                values=values.reshape(batch_size, num_samples, -1)\n",
    "                approx = null + torch.matmul(S, pred_eff)\n",
    "\n",
    "                loss2 = loss_fn2(approx, values)\n",
    "                loss2 = loss2 * 1\n",
    "                loss3 = loss_fn2(total, y-null)\n",
    "\n",
    "\n",
    "                ####################################################################################\n",
    "                ################### FORCE LOSS ON SINGLE FEATURE AT TIME ###########################\n",
    "                eye=torch.eye(self.num_players)\n",
    "                eye=eye.repeat(batch_size, 1).to(device)\n",
    "                x_extended=torch.cat([el.unsqueeze(0).repeat(self.num_players,1) for el in x])\n",
    "                single_pred=self.__call__(x_extended, eye)\n",
    "                single_pred=single_pred.reshape(batch_size*self.num_players, self.num_players, -1)\n",
    "                single_pred_sum = single_pred.sum(dim=1)\n",
    "                single_pred_surr=link(single_pred_sum)\n",
    "                single_pred_eff=additive_efficient_normalization(single_pred, single_pred_surr, null)\n",
    "                delta=single_pred_surr-null\n",
    "                tmp=[]\n",
    "                i=0\n",
    "                for el in single_pred_eff:\n",
    "                    tmp.append(el[i].unsqueeze(0))\n",
    "                    i+=1\n",
    "                    if i==self.num_players:\n",
    "                        i=0\n",
    "                tmp=torch.cat(tmp)\n",
    "                loss4=loss_fn2(tmp, delta)\n",
    "                ####################################################################################\n",
    "\n",
    "                loss5=loss_fn2(y,grand)\n",
    "\n",
    "                if iter<3:\n",
    "                    print(\"-\"*50)\n",
    "                    print(\"Y\",y)\n",
    "                    print(\"GRAND\",grand)\n",
    "                    print(\"TOAL\",total)\n",
    "\n",
    "                if epoch>=0:\n",
    "                    loss = loss1 + loss2*self.num_players #+ loss4*self.num_players + loss5*self.num_players\n",
    "                    if \"L3\" in loss_used:\n",
    "                        loss = loss + loss3*self.num_players\n",
    "                    if \"L4\" in loss_used:\n",
    "                        loss = loss + loss4*self.num_players\n",
    "                    if \"L5\" in loss_used:\n",
    "                        loss = loss + loss5*self.num_players\n",
    "                else:\n",
    "                    loss = loss1 \n",
    "                \n",
    "                lossprint = loss1.item() + loss2.item()  #+ loss4\n",
    "                if \"L3\" in loss_used:\n",
    "                    lossprint = lossprint + loss3.item() \n",
    "                if \"L4\" in loss_used:\n",
    "                    lossprint = lossprint + loss4.item() \n",
    "                if \"L5\" in loss_used:\n",
    "                    lossprint = lossprint + loss5.item() \n",
    "\n",
    "                N += len(x)\n",
    "                mean_loss += len(x) * (lossprint - mean_loss) / N\n",
    "                mean_loss1 += len(x) * (loss1.item()  - mean_loss1) / N\n",
    "                mean_loss2 += len(x) * (loss2.item()  - mean_loss2) / N\n",
    "                mean_loss3 += len(x) * (loss3.item()  - mean_loss3) / N\n",
    "                mean_loss4 += len(x) * (loss4.item()  - mean_loss4) / N\n",
    "                mean_loss5 += len(x) * (loss5.item()  - mean_loss5) / N\n",
    "\n",
    "                # Optimizer step.\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "\n",
    "                del loss, loss1, loss2, loss3, loss4, loss5\n",
    "                del values, pred\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if verbose:\n",
    "                print('----- Epoch = {} -----'.format(epoch + 1))\n",
    "\n",
    "                if epoch>=0:\n",
    "                    print('Train loss = {:.6f}'.format(mean_loss))\n",
    "                    print('Train loss1 = {:.6f}'.format(mean_loss1))\n",
    "                    print('Train loss2 = {:.6f}'.format(mean_loss2))\n",
    "                    print('Train loss3 = {:.10f}'.format(mean_loss3))\n",
    "                    print('Train loss4 = {:.6f}'.format(mean_loss4))\n",
    "                    print('Train loss5 = {:.6f}'.format(mean_loss5))\n",
    "                else:\n",
    "                    print('Train loss = {:.6f}'.format(mean_loss1))\n",
    "                    print('Train loss1 = {:.6f}'.format(mean_loss1))\n",
    "                    print('Train loss2 = -')\n",
    "                    print('Train loss3 = -')\n",
    "                    print('Train loss4 = -')\n",
    "                    print('Train loss5 = -')\n",
    "                    \n",
    "                print('')\n",
    "\n",
    "            # Evaluate validation loss.\n",
    "            self.model.eval()\n",
    "            val_loss, val_loss1, val_loss2, val_loss3, val_loss4, val_loss5 = validate_STFS(self, loss_fn1, loss_fn2, val_loader,  batch_size, num_samples, sampler, sampler_surr, paired_sampling, epoch, loss_used)#.item()\n",
    "            self.model.train()\n",
    "\n",
    "            # Print progress.\n",
    "            if verbose:\n",
    "                #print('----- Epoch = {} -----'.format(epoch + 1))\n",
    "                if epoch>=0:\n",
    "                    print('Val loss = {:.6f}'.format(val_loss))\n",
    "                    print('Val loss1 = {:.6f}'.format(val_loss1))\n",
    "                    print('Val loss2 = {:.6f}'.format(val_loss2))\n",
    "                    print('Val loss3 = {:.10f}'.format(val_loss3))\n",
    "                    print('Val loss4 = {:.6f}'.format(val_loss4))\n",
    "                    print('Val loss5 = {:.6f}'.format(val_loss5))\n",
    "                else:\n",
    "                    print('Val loss = {:.6f}'.format(val_loss1))\n",
    "                    print('Val loss1 = {:.6f}'.format(val_loss1))\n",
    "                    print('Val loss2 = -')\n",
    "                    print('Val loss3 = -')\n",
    "                    print('Val loss4 = -')\n",
    "                    print('Val loss5 = -')\n",
    "                print('')\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "            val_loss_list.append(val_loss)\n",
    "            val_loss1_list.append(val_loss1)\n",
    "            val_loss2_list.append(val_loss2)\n",
    "            val_loss3_list.append(val_loss3)\n",
    "            val_loss4_list.append(val_loss4)\n",
    "            val_loss5_list.append(val_loss5)\n",
    "            train_loss_list.append(mean_loss)\n",
    "            train_loss1_list.append(mean_loss1)\n",
    "            train_loss2_list.append(mean_loss2)\n",
    "            train_loss3_list.append(mean_loss3)\n",
    "            train_loss4_list.append(mean_loss4)\n",
    "            train_loss5_list.append(mean_loss5)\n",
    "\n",
    "            # Check if best model.\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model = deepcopy(model)\n",
    "                best_epoch = epoch\n",
    "                if verbose:\n",
    "                    print('\\t=> New best epoch, loss = {:.4f}'.format(val_loss))\n",
    "                    print('')\n",
    "            elif epoch - best_epoch == lookback:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "        # Clean up.\n",
    "        for param, best_param in zip(model.parameters(), best_model.parameters()):\n",
    "            param.data = best_param.data\n",
    "            \n",
    "        self.val_loss_list = val_loss_list\n",
    "        self.val_loss1_list = val_loss1_list\n",
    "        self.val_loss2_list = val_loss2_list\n",
    "        self.val_loss3_list = val_loss3_list\n",
    "        self.val_loss4_list = val_loss4_list\n",
    "        self.val_loss5_list = val_loss5_list\n",
    "        self.train_loss_list = train_loss_list\n",
    "        self.train_loss1_list = train_loss1_list\n",
    "        self.train_loss2_list = train_loss2_list\n",
    "        self.train_loss3_list = train_loss3_list\n",
    "        self.train_loss4_list = train_loss4_list\n",
    "        self.train_loss5_list = train_loss5_list\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def __call__(self, x, S):\n",
    "\n",
    "        return self.model((x,S))\n",
    "    \n",
    "\n",
    "    def shap_values(self, x):\n",
    "\n",
    "        # Data conversion.\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('data must be np.ndarray or torch.Tensor')\n",
    "\n",
    "        # Ensure null coalition is calculated.\n",
    "        device = next(self.model.parameters()).device\n",
    "        link=nn.Softmax(dim=-1)\n",
    "        x=x.to(device)\n",
    "        \n",
    "        # Generate explanations.\n",
    "        with torch.no_grad():\n",
    "            # Calculate grand coalition (for normalization).\n",
    "\n",
    "            zeros=torch.zeros(1, self.num_players, device=device)\n",
    "            null=self.__call__(x,zeros)\n",
    "            null_reshape = null.reshape(1, self.num_players, -1)\n",
    "            null_sum = null_reshape.sum(dim=1)\n",
    "            null=link(null_sum)\n",
    "            if len(null.shape) == 1:\n",
    "                null = null.reshape(1, 1)\n",
    "\n",
    "            ones=torch.ones(1, self.num_players, device=device)\n",
    "            pred=self.__call__(x, ones)\n",
    "            pred_reshape = pred.reshape(len(x), self.num_players, -1)\n",
    "            # grand_sum = pred_reshape.sum(dim=1)\n",
    "            # grand=link(grand_sum)\n",
    "\n",
    "            y=self.bbm(x)\n",
    "\n",
    "            pred = additive_efficient_normalization(pred_reshape, y, null)\n",
    "\n",
    "        return pred.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS_USED=[\"L1\",\"L2\"]\n",
    "# LOSS_USED=[\"L1\",\"L2\",\"L4\"]\n",
    "# LOSS_USED=[\"L1\",\"L2\",\"L5\"]\n",
    "LOSS_USED=[\"L1\",\"L2\",\"L4\", \"L5\"]\n",
    "LOSS_STRING=\"\".join(LOSS_USED)\n",
    "print(LOSS_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if os.path.isfile(f'checkpoints/{dataset}_lshap_{LOSS_STRING}.pt!'): ############################################################################\n",
    "    print('Loading saved explainer model')\n",
    "    net = torch.load(f'checkpoints/{dataset}_lshap.pt').to(device)\n",
    "    def original_model(x):\n",
    "        pred = bbm_model.predict_proba(x.cpu().numpy())\n",
    "        return torch.tensor(pred, dtype=torch.float32, device=x.device)\n",
    "    lshap = LightningSHAP(net, original_model, num_features)\n",
    "else:\n",
    "    LAYER_SIZE = 512 #1024\n",
    "    net = nn.Sequential(\n",
    "            MaskLayer1d(value=0, append=True),\n",
    "            nn.Linear(2*num_features, LAYER_SIZE),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(LAYER_SIZE, LAYER_SIZE),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(LAYER_SIZE, LAYER_SIZE),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            # nn.Linear(LAYER_SIZE, LAYER_SIZE),\n",
    "            # nn.LeakyReLU(inplace=True),\n",
    "            # nn.Linear(LAYER_SIZE, LAYER_SIZE),\n",
    "            # nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(LAYER_SIZE, 2 * num_features)\n",
    "        ).to(device)\n",
    "\n",
    "    # Set up original model\n",
    "    def original_model(x):\n",
    "        pred = bbm_model.predict_proba(x.cpu().numpy())\n",
    "        return torch.tensor(pred, dtype=torch.float32, device=x.device)\n",
    "\n",
    "    # Set up surrogate object\n",
    "    lshap = LightningSHAP(net, original_model, num_features)\n",
    "\n",
    "    # Train\n",
    "    start=time.time()\n",
    "    lshap.train_original_model(\n",
    "        X_train_s,\n",
    "        X_val_s[:200],\n",
    "        original_model,\n",
    "        batch_size=32,\n",
    "        num_samples=32,\n",
    "        paired_sampling=True,\n",
    "        max_epochs=1,\n",
    "        loss_fn1=KLDivLoss(), #KLDivLoss(),\n",
    "        loss_fn2=nn.MSELoss(), #KLDivLoss(),\n",
    "        validation_samples=32, #128\n",
    "        validation_batch_size=None,\n",
    "        lookback=10,\n",
    "        lr=5e-4,#2e-4\n",
    "        min_lr=1e-8,\n",
    "        weight_decay=1e-2, ######################################\n",
    "        lr_factor=0.5,\n",
    "        verbose=True,\n",
    "        training_seed=SEED,\n",
    "        loss_used=LOSS_USED\n",
    "        )\n",
    "    end=time.time()\n",
    "    print(\"Training Time:\",(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAPREG - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DefaultExtension:\n",
    "    '''Extend a model by replacing removed features with default values.'''\n",
    "    def __init__(self, values, model):\n",
    "        self.model = model\n",
    "        if values.ndim == 1:\n",
    "            values = values[np.newaxis]\n",
    "        elif values[0] != 1:\n",
    "            raise ValueError('values shape must be (dim,) or (1, dim)')\n",
    "        self.values = values\n",
    "        self.values_repeat = values\n",
    "\n",
    "    def __call__(self, x, S):\n",
    "        # Prepare x.\n",
    "        if len(x) != len(self.values_repeat):\n",
    "            self.values_repeat = self.values.repeat(len(x), 0)\n",
    "\n",
    "        # Replace specified indices.\n",
    "        x_ = x.copy()\n",
    "        x_[~S] = self.values_repeat[~S]\n",
    "\n",
    "        # Make predictions.\n",
    "        return self.model(x_)\n",
    "\n",
    "\n",
    "class MarginalExtension:\n",
    "    '''Extend a model by marginalizing out removed features using their\n",
    "    marginal distribution.'''\n",
    "    def __init__(self, data, model):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.data_repeat = data\n",
    "        self.samples = len(data)\n",
    "        # self.x_addr = None\n",
    "        # self.x_repeat = None\n",
    "\n",
    "    def __call__(self, x, S):\n",
    "        # Prepare x and S.\n",
    "        n = len(x)\n",
    "        x = x.repeat(self.samples, 0)\n",
    "        S = S.repeat(self.samples, 0)\n",
    "        # if self.x_addr != id(x):\n",
    "        #     self.x_addr = id(x)\n",
    "        #     self.x_repeat = x.repeat(self.samples, 0)\n",
    "        # x = self.x_repeat\n",
    "\n",
    "        # Prepare samples.\n",
    "        if len(self.data_repeat) != self.samples * n:\n",
    "            self.data_repeat = np.tile(self.data, (n, 1))\n",
    "\n",
    "        # Replace specified indices.\n",
    "        x_ = x.copy()\n",
    "        x_[~S] = self.data_repeat[~S]\n",
    "\n",
    "        # Make predictions.\n",
    "        pred = self.model(x_)\n",
    "        pred = pred.reshape(-1, self.samples, *pred.shape[1:])\n",
    "        return np.mean(pred, axis=1)\n",
    "\n",
    "\n",
    "class UniformExtension:\n",
    "    '''Extend a model by marginalizing out removed features using a\n",
    "    uniform distribution.'''\n",
    "    def __init__(self, values, categorical_inds, samples, model):\n",
    "        self.model = model\n",
    "        self.values = values\n",
    "        self.categorical_inds = categorical_inds\n",
    "        self.samples = samples\n",
    "\n",
    "    def __call__(self, x, S):\n",
    "        # Prepare x and S.\n",
    "        n = len(x)\n",
    "        x = x.repeat(self.samples, 0)\n",
    "        S = S.repeat(self.samples, 0)\n",
    "\n",
    "        # Prepare samples.\n",
    "        samples = np.zeros((n * self.samples, x.shape[1]))\n",
    "        for i in range(x.shape[1]):\n",
    "            if i in self.categorical_inds:\n",
    "                inds = np.random.choice(\n",
    "                    len(self.values[i]), n * self.samples)\n",
    "                samples[:, i] = self.values[i][inds]\n",
    "            else:\n",
    "                samples[:, i] = np.random.uniform(\n",
    "                    low=self.values[i][0], high=self.values[i][1],\n",
    "                    size=n * self.samples)\n",
    "\n",
    "        # Replace specified indices.\n",
    "        x_ = x.copy()\n",
    "        x_[~S] = samples[~S]\n",
    "\n",
    "        # Make predictions.\n",
    "        pred = self.model(x_)\n",
    "        pred = pred.reshape(-1, self.samples, *pred.shape[1:])\n",
    "        return np.mean(pred, axis=1)\n",
    "\n",
    "\n",
    "class UniformContinuousExtension:\n",
    "    '''\n",
    "    Extend a model by marginalizing out removed features using a\n",
    "    uniform distribution. Specific to sets of continuous features.\n",
    "\n",
    "    TODO: should we have caching here for repeating x?\n",
    "\n",
    "    '''\n",
    "    def __init__(self, min_vals, max_vals, samples, model):\n",
    "        self.model = model\n",
    "        self.min = min_vals\n",
    "        self.max = max_vals\n",
    "        self.samples = samples\n",
    "\n",
    "    def __call__(self, x, S):\n",
    "        # Prepare x and S.\n",
    "        x = x.repeat(self.samples, 0)\n",
    "        S = S.repeat(self.samples, 0)\n",
    "\n",
    "        # Prepare samples.\n",
    "        u = np.random.uniform(size=x.shape)\n",
    "        samples = u * self.min + (1 - u) * self.max\n",
    "\n",
    "        # Replace specified indices.\n",
    "        x_ = x.copy()\n",
    "        x_[~S] = samples[~S]\n",
    "\n",
    "        # Make predictions.\n",
    "        pred = self.model(x_)\n",
    "        pred = pred.reshape(-1, self.samples, *pred.shape[1:])\n",
    "        return np.mean(pred, axis=1)\n",
    "\n",
    "\n",
    "class ProductMarginalExtension:\n",
    "    '''Extend a model by marginalizing out removed features the\n",
    "    product of their marginal distributions.'''\n",
    "    def __init__(self, data, samples, model):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.data_repeat = data\n",
    "        self.samples = samples\n",
    "\n",
    "    def __call__(self, x, S):\n",
    "        # Prepare x and S.\n",
    "        n = len(x)\n",
    "        x = x.repeat(self.samples, 0)\n",
    "        S = S.repeat(self.samples, 0)\n",
    "\n",
    "        # Prepare samples.\n",
    "        samples = np.zeros((n * self.samples, x.shape[1]))\n",
    "        for i in range(x.shape[1]):\n",
    "            inds = np.random.choice(len(self.data), n * self.samples)\n",
    "            samples[:, i] = self.data[inds, i]\n",
    "\n",
    "        # Replace specified indices.\n",
    "        x_ = x.copy()\n",
    "        x_[~S] = samples[~S]\n",
    "\n",
    "        # Make predictions.\n",
    "        pred = self.model(x_)\n",
    "        pred = pred.reshape(-1, self.samples, *pred.shape[1:])\n",
    "        return np.mean(pred, axis=1)\n",
    "\n",
    "\n",
    "class SeparateModelExtension:\n",
    "    '''Extend a model using separate models for each subset of features.'''\n",
    "    def __init__(self, model_dict):\n",
    "        self.model_dict = model_dict\n",
    "\n",
    "    def __call__(self, x, S):\n",
    "        output = []\n",
    "        for i in range(len(S)):\n",
    "            # Extract model.\n",
    "            row = S[i]\n",
    "            model = self.model_dict[str(row)]\n",
    "\n",
    "            # Make prediction.\n",
    "            output.append(model(x[i:i+1, row]))\n",
    "\n",
    "        return np.concatenate(output, axis=0)\n",
    "\n",
    "\n",
    "class ConditionalExtension:\n",
    "    '''Extend a model by marginalizing out removed features using a model of\n",
    "    their conditional distribution.'''\n",
    "    def __init__(self, conditional_model, samples, model):\n",
    "        self.model = model\n",
    "        self.conditional_model = conditional_model\n",
    "        self.samples = samples\n",
    "        self.x_addr = None\n",
    "        self.x_repeat = None\n",
    "\n",
    "    def __call__(self, x, S):\n",
    "        # Prepare x.\n",
    "        if self.x_addr != id(x):\n",
    "            self.x_addr = id(x)\n",
    "            self.x_repeat = x.repeat(self.samples, 0)\n",
    "        x = self.x_repeat\n",
    "\n",
    "        # Prepare samples.\n",
    "        S = S.repeat(self.samples, 0)\n",
    "        samples = self.conditional_model(x, S)\n",
    "\n",
    "        # Replace specified indices.\n",
    "        x_ = x.copy()\n",
    "        x_[~S] = samples[~S]\n",
    "\n",
    "        # Make predictions.\n",
    "        pred = self.model(x_)\n",
    "        pred = pred.reshape(-1, self.samples, *pred.shape[1:])\n",
    "        return np.mean(pred, axis=1)\n",
    "\n",
    "\n",
    "class ConditionalSupervisedExtension:\n",
    "    '''Extend a model using a supervised surrogate model.'''\n",
    "    def __init__(self, surrogate):\n",
    "        self.surrogate = surrogate\n",
    "\n",
    "    def __call__(self, x, S):\n",
    "        return self.surrogate(x, S)\n",
    "\n",
    "\n",
    "def plot(shapley_values,\n",
    "         feature_names=None,\n",
    "         sort_features=True,\n",
    "         max_features=np.inf,\n",
    "         orientation='horizontal',\n",
    "         error_bars=True,\n",
    "         color='tab:green',\n",
    "         title='Feature Importance',\n",
    "         title_size=20,\n",
    "         tick_size=16,\n",
    "         tick_rotation=None,\n",
    "         axis_label='',\n",
    "         label_size=16,\n",
    "         figsize=(10, 7),\n",
    "         return_fig=False):\n",
    "    '''\n",
    "    Plot Shapley values.\n",
    "    Args:\n",
    "      shapley_values: ShapleyValues object.\n",
    "      feature_names: list of feature names.\n",
    "      sort_features: whether to sort features by their values.\n",
    "      max_features: number of features to display.\n",
    "      orientation: horizontal (default) or vertical.\n",
    "      error_bars: whether to include standard deviation error bars.\n",
    "      color: bar chart color.\n",
    "      title: plot title.\n",
    "      title_size: font size for title.\n",
    "      tick_size: font size for feature names and numerical values.\n",
    "      tick_rotation: tick rotation for feature names (vertical plots only).\n",
    "      label_size: font size for label.\n",
    "      figsize: figure size (if fig is None).\n",
    "      return_fig: whether to return matplotlib figure object.\n",
    "    '''\n",
    "    # Default feature names.\n",
    "    if feature_names is None:\n",
    "        feature_names = ['Feature {}'.format(i) for i in\n",
    "                         range(len(shapley_values.values))]\n",
    "\n",
    "    # Sort features if necessary.\n",
    "    if len(feature_names) > max_features:\n",
    "        sort_features = True\n",
    "\n",
    "    # Perform sorting.\n",
    "    values = shapley_values.values\n",
    "    std = shapley_values.std\n",
    "    if sort_features:\n",
    "        argsort = np.argsort(values)[::-1]\n",
    "        values = values[argsort]\n",
    "        std = std[argsort]\n",
    "        feature_names = np.array(feature_names)[argsort]\n",
    "\n",
    "    # Remove extra features if necessary.\n",
    "    if len(feature_names) > max_features:\n",
    "        feature_names = (list(feature_names[:max_features])\n",
    "                         + ['Remaining Features'])\n",
    "        values = (list(values[:max_features])\n",
    "                  + [np.sum(values[max_features:])])\n",
    "        std = (list(std[:max_features])\n",
    "               + [np.sum(std[max_features:] ** 2) ** 0.5])\n",
    "\n",
    "    # Warn if too many features.\n",
    "    if len(feature_names) > 50:\n",
    "        warnings.warn('Plotting {} features may make figure too crowded, '\n",
    "                      'consider using max_features'.format(\n",
    "                        len(feature_names)), Warning)\n",
    "\n",
    "    # Discard std if necessary.\n",
    "    if not error_bars:\n",
    "        std = None\n",
    "\n",
    "    # Make plot.\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.gca()\n",
    "\n",
    "    if orientation == 'horizontal':\n",
    "        # Bar chart.\n",
    "        ax.barh(np.arange(len(feature_names))[::-1], values,\n",
    "                color=color, xerr=std)\n",
    "\n",
    "        # Feature labels.\n",
    "        if tick_rotation is not None:\n",
    "            raise ValueError('rotation not supported for horizontal charts')\n",
    "        ax.set_yticks(np.arange(len(feature_names))[::-1])\n",
    "        ax.set_yticklabels(feature_names, fontsize=label_size)\n",
    "\n",
    "        # Axis labels and ticks.\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel(axis_label, fontsize=label_size)\n",
    "        ax.tick_params(axis='x', labelsize=tick_size)\n",
    "\n",
    "    elif orientation == 'vertical':\n",
    "        # Bar chart.\n",
    "        ax.bar(np.arange(len(feature_names)), values, color=color,\n",
    "               yerr=std)\n",
    "\n",
    "        # Feature labels.\n",
    "        if tick_rotation is None:\n",
    "            tick_rotation = 45\n",
    "        if tick_rotation < 90:\n",
    "            ha = 'right'\n",
    "            rotation_mode = 'anchor'\n",
    "        else:\n",
    "            ha = 'center'\n",
    "            rotation_mode = 'default'\n",
    "        ax.set_xticks(np.arange(len(feature_names)))\n",
    "        ax.set_xticklabels(feature_names, rotation=tick_rotation, ha=ha,\n",
    "                           rotation_mode=rotation_mode,\n",
    "                           fontsize=label_size)\n",
    "\n",
    "        # Axis labels and ticks.\n",
    "        ax.set_ylabel(axis_label, fontsize=label_size)\n",
    "        ax.set_xlabel('')\n",
    "        ax.tick_params(axis='y', labelsize=tick_size)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('orientation must be horizontal or vertical')\n",
    "\n",
    "    # Remove spines.\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    ax.set_title(title, fontsize=title_size)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if return_fig:\n",
    "        return fig\n",
    "    else:\n",
    "        return\n",
    "\n",
    "\n",
    "def comparison_plot(comparison_values,\n",
    "                    comparison_names=None,\n",
    "                    feature_names=None,\n",
    "                    sort_features=True,\n",
    "                    max_features=np.inf,\n",
    "                    orientation='vertical',\n",
    "                    error_bars=True,\n",
    "                    colors=('tab:green', 'tab:blue'),\n",
    "                    title='Feature Importance Comparison',\n",
    "                    title_size=20,\n",
    "                    tick_size=16,\n",
    "                    tick_rotation=None,\n",
    "                    axis_label='',\n",
    "                    label_size=16,\n",
    "                    legend_loc=None,\n",
    "                    figsize=(10, 7),\n",
    "                    return_fig=False):\n",
    "    '''\n",
    "    Plot comparison between two different ShapleyValues objects.\n",
    "    Args:\n",
    "      comparison_values: tuple of ShapleyValues objects to be compared.\n",
    "      comparison_names: tuple of names for each ShapleyValues object.\n",
    "      feature_names: list of feature names.\n",
    "      sort_features: whether to sort features by their Shapley values.\n",
    "      max_features: number of features to display.\n",
    "      orientation: horizontal (default) or vertical.\n",
    "      error_bars: whether to include standard deviation error bars.\n",
    "      colors: colors for each set of Shapley values.\n",
    "      title: plot title.\n",
    "      title_size: font size for title.\n",
    "      tick_size: font size for feature names and numerical values.\n",
    "      tick_rotation: tick rotation for feature names (vertical plots only).\n",
    "      label_size: font size for label.\n",
    "      legend_loc: legend location.\n",
    "      figsize: figure size (if fig is None).\n",
    "      return_fig: whether to return matplotlib figure object.\n",
    "    '''\n",
    "    # Default feature names.\n",
    "    if feature_names is None:\n",
    "        feature_names = ['Feature {}'.format(i) for i in\n",
    "                         range(len(comparison_values[0].values))]\n",
    "\n",
    "    # Default comparison names.\n",
    "    num_comps = len(comparison_values)\n",
    "    if num_comps not in (2, 3, 4, 5):\n",
    "        raise ValueError('only support comparisons for 2-5 sets of values')\n",
    "    if comparison_names is None:\n",
    "        comparison_names = ['Shapley Values {}'.format(i) for i in\n",
    "                            range(num_comps)]\n",
    "\n",
    "    # Default colors.\n",
    "    if colors is None:\n",
    "        colors = ['tab:green', 'tab:blue', 'tab:purple',\n",
    "                  'tab:orange', 'tab:pink'][:num_comps]\n",
    "\n",
    "    # Sort features if necessary.\n",
    "    if len(feature_names) > max_features:\n",
    "        sort_features = True\n",
    "\n",
    "    # Extract values.\n",
    "    values = [shapley_values.values for shapley_values in comparison_values]\n",
    "    std = [shapley_values.std for shapley_values in comparison_values]\n",
    "\n",
    "    # Perform sorting.\n",
    "    if sort_features:\n",
    "        argsort = np.argsort(values[0])[::-1]\n",
    "        values = [shapley_values[argsort] for shapley_values in values]\n",
    "        std = [stddev[argsort] for stddev in std]\n",
    "        feature_names = np.array(feature_names)[argsort]\n",
    "\n",
    "    # Remove extra features if necessary.\n",
    "    if len(feature_names) > max_features:\n",
    "        feature_names = (list(feature_names[:max_features])\n",
    "                         + ['Remaining Features'])\n",
    "        values = [\n",
    "            list(shapley_values[:max_features])\n",
    "            + [np.sum(shapley_values[max_features:])]\n",
    "            for shapley_values in values]\n",
    "        std = [list(stddev[:max_features])\n",
    "               + [np.sum(stddev[max_features:] ** 2) ** 0.5]\n",
    "               for stddev in std]\n",
    "\n",
    "    # Warn if too many features.\n",
    "    if len(feature_names) > 50:\n",
    "        warnings.warn('Plotting {} features may make figure too crowded, '\n",
    "                      'consider using max_features'.format(\n",
    "                        len(feature_names)), Warning)\n",
    "\n",
    "    # Discard std if necessary.\n",
    "    if not error_bars:\n",
    "        std = [None for _ in std]\n",
    "\n",
    "    # Make plot.\n",
    "    width = 0.8 / num_comps\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.gca()\n",
    "\n",
    "    if orientation == 'horizontal':\n",
    "        # Bar chart.\n",
    "        enumeration = enumerate(zip(values, std, comparison_names, colors))\n",
    "        for i, (shapley_values, stddev, name, color) in enumeration:\n",
    "            pos = - 0.4 + width / 2 + width * i\n",
    "            ax.barh(np.arange(len(feature_names))[::-1] - pos,\n",
    "                    shapley_values, height=width, color=color, xerr=stddev,\n",
    "                    label=name)\n",
    "\n",
    "        # Feature labels.\n",
    "        if tick_rotation is not None:\n",
    "            raise ValueError('rotation not supported for horizontal charts')\n",
    "        ax.set_yticks(np.arange(len(feature_names))[::-1])\n",
    "        ax.set_yticklabels(feature_names, fontsize=label_size)\n",
    "\n",
    "        # Axis labels and ticks.\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel(axis_label, fontsize=label_size)\n",
    "        ax.tick_params(axis='x', labelsize=tick_size)\n",
    "\n",
    "    elif orientation == 'vertical':\n",
    "        # Bar chart.\n",
    "        enumeration = enumerate(zip(values, std, comparison_names, colors))\n",
    "        for i, (shapley_values, stddev, name, color) in enumeration:\n",
    "            pos = - 0.4 + width / 2 + width * i\n",
    "            ax.bar(np.arange(len(feature_names)) + pos,\n",
    "                   shapley_values, width=width, color=color, yerr=stddev,\n",
    "                   label=name)\n",
    "\n",
    "        # Feature labels.\n",
    "        if tick_rotation is None:\n",
    "            tick_rotation = 45\n",
    "        if tick_rotation < 90:\n",
    "            ha = 'right'\n",
    "            rotation_mode = 'anchor'\n",
    "        else:\n",
    "            ha = 'center'\n",
    "            rotation_mode = 'default'\n",
    "        ax.set_xticks(np.arange(len(feature_names)))\n",
    "        ax.set_xticklabels(feature_names, rotation=tick_rotation, ha=ha,\n",
    "                           rotation_mode=rotation_mode,\n",
    "                           fontsize=label_size)\n",
    "\n",
    "        # Axis labels and ticks.\n",
    "        ax.set_ylabel(axis_label, fontsize=label_size)\n",
    "        ax.set_xlabel('')\n",
    "        ax.tick_params(axis='y', labelsize=tick_size)\n",
    "\n",
    "    # Remove spines.\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    plt.legend(loc=legend_loc, fontsize=label_size)\n",
    "    ax.set_title(title, fontsize=title_size)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if return_fig:\n",
    "        return fig\n",
    "    else:\n",
    "        return\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def crossentropyloss(pred, target):\n",
    "    '''Cross entropy loss that does not average across samples.'''\n",
    "    if pred.ndim == 1:\n",
    "        pred = pred[:, np.newaxis]\n",
    "        pred = np.concatenate((1 - pred, pred), axis=1)\n",
    "\n",
    "    if pred.shape == target.shape:\n",
    "        # Soft cross entropy loss.\n",
    "        pred = np.clip(pred, a_min=1e-12, a_max=1-1e-12)\n",
    "        return - np.sum(np.log(pred) * target, axis=1)\n",
    "    else:\n",
    "        # Standard cross entropy loss.\n",
    "        return - np.log(pred[np.arange(len(pred)), target])\n",
    "\n",
    "\n",
    "def mseloss(pred, target):\n",
    "    '''MSE loss that does not average across samples.'''\n",
    "    if len(pred.shape) == 1:\n",
    "        pred = pred[:, np.newaxis]\n",
    "    if len(target.shape) == 1:\n",
    "        target = target[:, np.newaxis]\n",
    "    return np.sum((pred - target) ** 2, axis=1)\n",
    "\n",
    "\n",
    "class ShapleyValues:\n",
    "    '''For storing and plotting Shapley values.'''\n",
    "    def __init__(self, values, std):\n",
    "        self.values = values\n",
    "        self.std = std\n",
    "\n",
    "    def plot(self,\n",
    "             feature_names=None,\n",
    "             sort_features=True,\n",
    "             max_features=np.inf,\n",
    "             orientation='horizontal',\n",
    "             error_bars=True,\n",
    "             color='C0',\n",
    "             title='Feature Importance',\n",
    "             title_size=20,\n",
    "             tick_size=16,\n",
    "             tick_rotation=None,\n",
    "             axis_label='',\n",
    "             label_size=16,\n",
    "             figsize=(10, 7),\n",
    "             return_fig=False):\n",
    "        '''\n",
    "        Plot Shapley values.\n",
    "        Args:\n",
    "          feature_names: list of feature names.\n",
    "          sort_features: whether to sort features by their Shapley values.\n",
    "          max_features: number of features to display.\n",
    "          orientation: horizontal (default) or vertical.\n",
    "          error_bars: whether to include standard deviation error bars.\n",
    "          color: bar chart color.\n",
    "          title: plot title.\n",
    "          title_size: font size for title.\n",
    "          tick_size: font size for feature names and numerical values.\n",
    "          tick_rotation: tick rotation for feature names (vertical plots only).\n",
    "          label_size: font size for label.\n",
    "          figsize: figure size (if fig is None).\n",
    "          return_fig: whether to return matplotlib figure object.\n",
    "        '''\n",
    "        return plotting.plot(\n",
    "            self, feature_names, sort_features, max_features, orientation,\n",
    "            error_bars, color, title, title_size, tick_size, tick_rotation,\n",
    "            axis_label, label_size, figsize, return_fig)\n",
    "\n",
    "    def comparison(self,\n",
    "                   other_values,\n",
    "                   comparison_names=None,\n",
    "                   feature_names=None,\n",
    "                   sort_features=True,\n",
    "                   max_features=np.inf,\n",
    "                   orientation='vertical',\n",
    "                   error_bars=True,\n",
    "                   colors=None,\n",
    "                   title='Shapley Value Comparison',\n",
    "                   title_size=20,\n",
    "                   tick_size=16,\n",
    "                   tick_rotation=None,\n",
    "                   axis_label='',\n",
    "                   label_size=16,\n",
    "                   legend_loc=None,\n",
    "                   figsize=(10, 7),\n",
    "                   return_fig=False):\n",
    "        '''\n",
    "        Plot comparison with another set of Shapley values.\n",
    "        Args:\n",
    "          other_values: another Shapley values object.\n",
    "          comparison_names: tuple of names for each Shapley value object.\n",
    "          feature_names: list of feature names.\n",
    "          sort_features: whether to sort features by their Shapley values.\n",
    "          max_features: number of features to display.\n",
    "          orientation: horizontal (default) or vertical.\n",
    "          error_bars: whether to include standard deviation error bars.\n",
    "          colors: colors for each set of Shapley values.\n",
    "          title: plot title.\n",
    "          title_size: font size for title.\n",
    "          tick_size: font size for feature names and numerical values.\n",
    "          tick_rotation: tick rotation for feature names (vertical plots only).\n",
    "          label_size: font size for label.\n",
    "          legend_loc: legend location.\n",
    "          figsize: figure size (if fig is None).\n",
    "          return_fig: whether to return matplotlib figure object.\n",
    "        '''\n",
    "        return plotting.comparison_plot(\n",
    "            (self, other_values), comparison_names, feature_names,\n",
    "            sort_features, max_features, orientation, error_bars, colors, title,\n",
    "            title_size, tick_size, tick_rotation, axis_label, label_size,\n",
    "            legend_loc, figsize, return_fig)\n",
    "\n",
    "    def save(self, filename):\n",
    "        '''Save Shapley values object.'''\n",
    "        if isinstance(filename, str):\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(self, f)\n",
    "        else:\n",
    "            raise TypeError('filename must be str')\n",
    "\n",
    "    def __repr__(self):\n",
    "        with np.printoptions(precision=2, threshold=12, floatmode='fixed'):\n",
    "            return 'Shapley Values(\\n  (Mean): {}\\n  (Std):  {}\\n)'.format(\n",
    "                self.values, self.std)\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    '''Load Shapley values object.'''\n",
    "    with open(filename, 'rb') as f:\n",
    "        shapley_values = pickle.load(f)\n",
    "        if isinstance(shapley_values, ShapleyValues):\n",
    "            return shapley_values\n",
    "        else:\n",
    "            raise ValueError('object is not instance of ShapleyValues class')\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CooperativeGame:\n",
    "    '''Base class for cooperative games.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, S):\n",
    "        '''Evaluate cooperative game.'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def grand(self):\n",
    "        '''Get grand coalition value.'''\n",
    "        return self.__call__(np.ones((1, self.players), dtype=bool))[0]\n",
    "\n",
    "    def null(self):\n",
    "        '''Get null coalition value.'''\n",
    "        return self.__call__(np.zeros((1, self.players), dtype=bool))[0]\n",
    "\n",
    "\n",
    "class PredictionGame(CooperativeGame):\n",
    "    '''\n",
    "    Cooperative game for an individual example's prediction.\n",
    "    Args:\n",
    "      extension: model extension (see removal.py).\n",
    "      sample: numpy array representing a single model input.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, extension, sample, groups=None):\n",
    "        # Add batch dimension to sample.\n",
    "        if sample.ndim == 1:\n",
    "            sample = sample[np.newaxis]\n",
    "        elif sample.shape[0] != 1:\n",
    "            raise ValueError('sample must have shape (ndim,) or (1,ndim)')\n",
    "\n",
    "        self.extension = extension\n",
    "        self.sample = sample\n",
    "\n",
    "        # Store feature groups.\n",
    "        num_features = sample.shape[1]\n",
    "        if groups is None:\n",
    "            self.players = num_features\n",
    "            self.groups_matrix = None\n",
    "        else:\n",
    "            # Verify groups.\n",
    "            inds_list = []\n",
    "            for group in groups:\n",
    "                inds_list += list(group)\n",
    "            assert np.all(np.sort(inds_list) == np.arange(num_features))\n",
    "\n",
    "            # Map groups to features.\n",
    "            self.players = len(groups)\n",
    "            self.groups_matrix = np.zeros(\n",
    "                (len(groups), num_features), dtype=bool)\n",
    "            for i, group in enumerate(groups):\n",
    "                self.groups_matrix[i, group] = True\n",
    "\n",
    "        # Caching.\n",
    "        self.sample_repeat = sample\n",
    "\n",
    "    def __call__(self, S):\n",
    "        '''\n",
    "        Evaluate cooperative game.\n",
    "        Args:\n",
    "          S: array of player coalitions with size (batch, players).\n",
    "        '''\n",
    "        # Try to use caching for repeated data.\n",
    "        if len(S) != len(self.sample_repeat):\n",
    "            self.sample_repeat = self.sample.repeat(len(S), 0)\n",
    "        input_data = self.sample_repeat\n",
    "\n",
    "        # Apply group transformation.\n",
    "        if self.groups_matrix is not None:\n",
    "            S = np.matmul(S, self.groups_matrix)\n",
    "\n",
    "        # Evaluate.\n",
    "        return self.extension(input_data, S)\n",
    "\n",
    "\n",
    "class PredictionLossGame(CooperativeGame):\n",
    "    '''\n",
    "    Cooperative game for an individual example's loss value.\n",
    "    Args:\n",
    "      extension: model extension (see removal.py).\n",
    "      sample: numpy array representing a single model input.\n",
    "      label: the input's true label.\n",
    "      loss: loss function (see utils.py).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, extension, sample, label, loss, groups=None):\n",
    "        # Add batch dimension to sample.\n",
    "        if sample.ndim == 1:\n",
    "            sample = sample[np.newaxis]\n",
    "\n",
    "        # Add batch dimension to label.\n",
    "        if np.isscalar(label):\n",
    "            label = np.array([label])\n",
    "\n",
    "        # Convert label dtype if necessary.\n",
    "        if loss is crossentropyloss:\n",
    "            # Make sure not soft cross entropy.\n",
    "            if (label.ndim <= 1) or (label.shape[1] == 1):\n",
    "                # Only convert if float.\n",
    "                if np.issubdtype(label.dtype, np.floating):\n",
    "                    label = label.astype(int)\n",
    "\n",
    "        self.extension = extension\n",
    "        self.sample = sample\n",
    "        self.label = label\n",
    "        self.loss = loss\n",
    "\n",
    "        # Store feature groups.\n",
    "        num_features = sample.shape[1]\n",
    "        if groups is None:\n",
    "            self.players = num_features\n",
    "            self.groups_matrix = None\n",
    "        else:\n",
    "            # Verify groups.\n",
    "            inds_list = []\n",
    "            for group in groups:\n",
    "                inds_list += list(group)\n",
    "            assert np.all(np.sort(inds_list) == np.arange(num_features))\n",
    "\n",
    "            # Map groups to features.\n",
    "            self.players = len(groups)\n",
    "            self.groups_matrix = np.zeros(\n",
    "                (len(groups), num_features), dtype=bool)\n",
    "            for i, group in enumerate(groups):\n",
    "                self.groups_matrix[i, group] = True\n",
    "\n",
    "        # Caching.\n",
    "        self.sample_repeat = sample\n",
    "        self.label_repeat = label\n",
    "\n",
    "    def __call__(self, S):\n",
    "        '''\n",
    "        Evaluate cooperative game.\n",
    "        Args:\n",
    "          S: array of player coalitions with size (batch, players).\n",
    "        '''\n",
    "        # Try to use caching for repeated data.\n",
    "        if len(S) != len(self.sample_repeat):\n",
    "            self.sample_repeat = self.sample.repeat(len(S), 0)\n",
    "            self.label_repeat = self.label.repeat(len(S), 0)\n",
    "        input_data = self.sample_repeat\n",
    "        output_label = self.label_repeat\n",
    "\n",
    "        # Apply group transformation.\n",
    "        if self.groups_matrix is not None:\n",
    "            S = np.matmul(S, self.groups_matrix)\n",
    "\n",
    "        # Evaluate.\n",
    "        return - self.loss(self.extension(input_data, S), output_label)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CooperativeGame:\n",
    "    '''Base class for cooperative games.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, S):\n",
    "        '''Evaluate cooperative game.'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def grand(self):\n",
    "        '''Get grand coalition value.'''\n",
    "        return self.__call__(np.ones((1, self.players), dtype=bool))[0]\n",
    "\n",
    "    def null(self):\n",
    "        '''Get null coalition value.'''\n",
    "        return self.__call__(np.zeros((1, self.players), dtype=bool))[0]\n",
    "\n",
    "\n",
    "class PredictionGame(CooperativeGame):\n",
    "    '''\n",
    "    Cooperative game for an individual example's prediction.\n",
    "    Args:\n",
    "      extension: model extension (see removal.py).\n",
    "      sample: numpy array representing a single model input.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, extension, sample, groups=None):\n",
    "        # Add batch dimension to sample.\n",
    "        if sample.ndim == 1:\n",
    "            sample = sample[np.newaxis]\n",
    "        elif sample.shape[0] != 1:\n",
    "            raise ValueError('sample must have shape (ndim,) or (1,ndim)')\n",
    "\n",
    "        self.extension = extension\n",
    "        self.sample = sample\n",
    "\n",
    "        # Store feature groups.\n",
    "        num_features = sample.shape[1]\n",
    "        if groups is None:\n",
    "            self.players = num_features\n",
    "            self.groups_matrix = None\n",
    "        else:\n",
    "            # Verify groups.\n",
    "            inds_list = []\n",
    "            for group in groups:\n",
    "                inds_list += list(group)\n",
    "            assert np.all(np.sort(inds_list) == np.arange(num_features))\n",
    "\n",
    "            # Map groups to features.\n",
    "            self.players = len(groups)\n",
    "            self.groups_matrix = np.zeros(\n",
    "                (len(groups), num_features), dtype=bool)\n",
    "            for i, group in enumerate(groups):\n",
    "                self.groups_matrix[i, group] = True\n",
    "\n",
    "        # Caching.\n",
    "        self.sample_repeat = sample\n",
    "\n",
    "    def __call__(self, S):\n",
    "        '''\n",
    "        Evaluate cooperative game.\n",
    "        Args:\n",
    "          S: array of player coalitions with size (batch, players).\n",
    "        '''\n",
    "        # Try to use caching for repeated data.\n",
    "        if len(S) != len(self.sample_repeat):\n",
    "            self.sample_repeat = self.sample.repeat(len(S), 0)\n",
    "        input_data = self.sample_repeat\n",
    "\n",
    "        # Apply group transformation.\n",
    "        if self.groups_matrix is not None:\n",
    "            S = np.matmul(S, self.groups_matrix)\n",
    "\n",
    "        # Evaluate.\n",
    "        return self.extension(input_data, S)\n",
    "\n",
    "\n",
    "class PredictionLossGame(CooperativeGame):\n",
    "    '''\n",
    "    Cooperative game for an individual example's loss value.\n",
    "    Args:\n",
    "      extension: model extension (see removal.py).\n",
    "      sample: numpy array representing a single model input.\n",
    "      label: the input's true label.\n",
    "      loss: loss function (see utils.py).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, extension, sample, label, loss, groups=None):\n",
    "        # Add batch dimension to sample.\n",
    "        if sample.ndim == 1:\n",
    "            sample = sample[np.newaxis]\n",
    "\n",
    "        # Add batch dimension to label.\n",
    "        if np.isscalar(label):\n",
    "            label = np.array([label])\n",
    "\n",
    "        # Convert label dtype if necessary.\n",
    "        if loss is crossentropyloss:\n",
    "            # Make sure not soft cross entropy.\n",
    "            if (label.ndim <= 1) or (label.shape[1] == 1):\n",
    "                # Only convert if float.\n",
    "                if np.issubdtype(label.dtype, np.floating):\n",
    "                    label = label.astype(int)\n",
    "\n",
    "        self.extension = extension\n",
    "        self.sample = sample\n",
    "        self.label = label\n",
    "        self.loss = loss\n",
    "\n",
    "        # Store feature groups.\n",
    "        num_features = sample.shape[1]\n",
    "        if groups is None:\n",
    "            self.players = num_features\n",
    "            self.groups_matrix = None\n",
    "        else:\n",
    "            # Verify groups.\n",
    "            inds_list = []\n",
    "            for group in groups:\n",
    "                inds_list += list(group)\n",
    "            assert np.all(np.sort(inds_list) == np.arange(num_features))\n",
    "\n",
    "            # Map groups to features.\n",
    "            self.players = len(groups)\n",
    "            self.groups_matrix = np.zeros(\n",
    "                (len(groups), num_features), dtype=bool)\n",
    "            for i, group in enumerate(groups):\n",
    "                self.groups_matrix[i, group] = True\n",
    "\n",
    "        # Caching.\n",
    "        self.sample_repeat = sample\n",
    "        self.label_repeat = label\n",
    "\n",
    "    def __call__(self, S):\n",
    "        '''\n",
    "        Evaluate cooperative game.\n",
    "        Args:\n",
    "          S: array of player coalitions with size (batch, players).\n",
    "        '''\n",
    "        # Try to use caching for repeated data.\n",
    "        if len(S) != len(self.sample_repeat):\n",
    "            self.sample_repeat = self.sample.repeat(len(S), 0)\n",
    "            self.label_repeat = self.label.repeat(len(S), 0)\n",
    "        input_data = self.sample_repeat\n",
    "        output_label = self.label_repeat\n",
    "\n",
    "        # Apply group transformation.\n",
    "        if self.groups_matrix is not None:\n",
    "            S = np.matmul(S, self.groups_matrix)\n",
    "\n",
    "        # Evaluate.\n",
    "        return - self.loss(self.extension(input_data, S), output_label)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def default_min_variance_samples(game):\n",
    "    '''Determine min_variance_samples.'''\n",
    "    return 5\n",
    "\n",
    "\n",
    "def default_variance_batches(game, batch_size):\n",
    "    '''\n",
    "    Determine variance_batches.\n",
    "    This value tries to ensure that enough samples are included to make A\n",
    "    approximation non-singular.\n",
    "    '''\n",
    "    if isinstance(game, CooperativeGame):\n",
    "        return int(np.ceil(10 * game.players / batch_size))\n",
    "    else:\n",
    "        # Require more intermediate samples for stochastic games.\n",
    "        return int(np.ceil(25 * game.players / batch_size))\n",
    "\n",
    "\n",
    "def calculate_result(A, b, total):\n",
    "    '''Calculate the regression coefficients.'''\n",
    "    num_players = A.shape[1]\n",
    "    try:\n",
    "        if len(b.shape) == 2:\n",
    "            A_inv_one = np.linalg.solve(A, np.ones((num_players, 1)))\n",
    "        else:\n",
    "            A_inv_one = np.linalg.solve(A, np.ones(num_players))\n",
    "        A_inv_vec = np.linalg.solve(A, b)\n",
    "        values = (\n",
    "            A_inv_vec -\n",
    "            A_inv_one * (np.sum(A_inv_vec, axis=0, keepdims=True) - total)\n",
    "            / np.sum(A_inv_one))\n",
    "    except np.linalg.LinAlgError:\n",
    "        raise ValueError('singular matrix inversion. Consider using larger '\n",
    "                         'variance_batches')\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def ShapleyRegression(game,\n",
    "                      batch_size=512,\n",
    "                      detect_convergence=True,\n",
    "                      thresh=0.01,\n",
    "                      n_samples=None,\n",
    "                      paired_sampling=True,\n",
    "                      return_all=False,\n",
    "                      min_variance_samples=None,\n",
    "                      variance_batches=None,\n",
    "                      bar=True,\n",
    "                      verbose=False):\n",
    "    # Verify arguments.\n",
    "    if isinstance(game, CooperativeGame):\n",
    "        stochastic = False\n",
    "    elif isinstance(game, StochasticCooperativeGame):\n",
    "        stochastic = True\n",
    "    else:\n",
    "        raise ValueError('game must be CooperativeGame or '\n",
    "                         'StochasticCooperativeGame')\n",
    "\n",
    "    if min_variance_samples is None:\n",
    "        min_variance_samples = default_min_variance_samples(game)\n",
    "    else:\n",
    "        assert isinstance(min_variance_samples, int)\n",
    "        assert min_variance_samples > 1\n",
    "\n",
    "    if variance_batches is None:\n",
    "        variance_batches = default_variance_batches(game, batch_size)\n",
    "    else:\n",
    "        assert isinstance(variance_batches, int)\n",
    "        assert variance_batches >= 1\n",
    "\n",
    "    # Possibly force convergence detection.\n",
    "    if n_samples is None:\n",
    "        n_samples = 1e20\n",
    "        if not detect_convergence:\n",
    "            detect_convergence = True\n",
    "            if verbose:\n",
    "                print('Turning convergence detection on')\n",
    "\n",
    "    if detect_convergence:\n",
    "        assert 0 < thresh < 1\n",
    "\n",
    "    # Weighting kernel (probability of each subset size).\n",
    "    num_players = game.players\n",
    "    weights = np.arange(1, num_players)\n",
    "    weights = 1 / (weights * (num_players - weights))\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    # Calculate null and grand coalitions for constraints.\n",
    "    if stochastic:\n",
    "        null = game.null(batch_size=batch_size)\n",
    "        grand = game.grand(batch_size=batch_size)\n",
    "    else:\n",
    "        null = game.null()\n",
    "        grand = game.grand()\n",
    "\n",
    "    # Calculate difference between grand and null coalitions.\n",
    "    total = grand - null\n",
    "\n",
    "    # Set up bar.\n",
    "    n_loops = int(np.ceil(n_samples / batch_size))\n",
    "    if bar:\n",
    "        if detect_convergence:\n",
    "            bar = tqdm(total=1)\n",
    "        else:\n",
    "            bar = tqdm(total=n_loops * batch_size)\n",
    "\n",
    "    # Setup.\n",
    "    n = 0\n",
    "    b = 0\n",
    "    A = 0\n",
    "    estimate_list = []\n",
    "\n",
    "    # For variance estimation.\n",
    "    A_sample_list = []\n",
    "    b_sample_list = []\n",
    "\n",
    "    # For tracking progress.\n",
    "    var = np.nan * np.ones(num_players)\n",
    "    if return_all:\n",
    "        N_list = []\n",
    "        std_list = []\n",
    "        val_list = []\n",
    "\n",
    "    # Begin sampling.\n",
    "    for it in range(n_loops):\n",
    "        # Sample subsets.\n",
    "        S = np.zeros((batch_size, num_players), dtype=bool)\n",
    "        num_included = np.random.choice(num_players - 1, size=batch_size,\n",
    "                                        p=weights) + 1\n",
    "        for row, num in zip(S, num_included):\n",
    "            inds = np.random.choice(num_players, size=num, replace=False)\n",
    "            row[inds] = 1\n",
    "\n",
    "        # Sample exogenous (if applicable).\n",
    "        if stochastic:\n",
    "            U = game.sample(batch_size)\n",
    "\n",
    "        # Update estimators.\n",
    "        if paired_sampling:\n",
    "            # Paired samples.\n",
    "            A_sample = 0.5 * (\n",
    "                np.matmul(S[:, :, np.newaxis].astype(float),\n",
    "                          S[:, np.newaxis, :].astype(float))\n",
    "                + np.matmul(np.logical_not(S)[:, :, np.newaxis].astype(float),\n",
    "                            np.logical_not(S)[:, np.newaxis, :].astype(float)))\n",
    "            if stochastic:\n",
    "                game_eval = game(S, U) - null\n",
    "                S_comp = np.logical_not(S)\n",
    "                comp_eval = game(S_comp, U) - null\n",
    "                b_sample = 0.5 * (\n",
    "                    S.astype(float).T * game_eval[:, np.newaxis].T\n",
    "                    + S_comp.astype(float).T * comp_eval[:, np.newaxis].T).T\n",
    "            else:\n",
    "                game_eval = game(S) - null\n",
    "                S_comp = np.logical_not(S)\n",
    "                comp_eval = game(S_comp) - null\n",
    "                b_sample = 0.5 * (\n",
    "                    S.astype(float).T * game_eval[:, np.newaxis].T +\n",
    "                    S_comp.astype(float).T * comp_eval[:, np.newaxis].T).T\n",
    "        else:\n",
    "            # Single sample.\n",
    "            A_sample = np.matmul(S[:, :, np.newaxis].astype(float),\n",
    "                                 S[:, np.newaxis, :].astype(float))\n",
    "            if stochastic:\n",
    "                b_sample = (S.astype(float).T\n",
    "                            * (game(S, U) - null)[:, np.newaxis].T).T\n",
    "            else:\n",
    "                b_sample = (S.astype(float).T\n",
    "                            * (game(S) - null)[:, np.newaxis].T).T\n",
    "\n",
    "        # Welford's algorithm.\n",
    "        n += batch_size\n",
    "        b += np.sum(b_sample - b, axis=0) / n\n",
    "        A += np.sum(A_sample - A, axis=0) / n\n",
    "\n",
    "        # Calculate progress.\n",
    "        values = calculate_result(A, b, total)\n",
    "        A_sample_list.append(A_sample)\n",
    "        b_sample_list.append(b_sample)\n",
    "        if len(A_sample_list) == variance_batches:\n",
    "            # Aggregate samples for intermediate estimate.\n",
    "            A_sample = np.concatenate(A_sample_list, axis=0).mean(axis=0)\n",
    "            b_sample = np.concatenate(b_sample_list, axis=0).mean(axis=0)\n",
    "            A_sample_list = []\n",
    "            b_sample_list = []\n",
    "\n",
    "            # Add new estimate.\n",
    "            estimate_list.append(calculate_result(A_sample, b_sample, total))\n",
    "\n",
    "            # Estimate current var.\n",
    "            if len(estimate_list) >= min_variance_samples:\n",
    "                var = np.array(estimate_list).var(axis=0)\n",
    "\n",
    "        # Convergence ratio.\n",
    "        std = np.sqrt(var * variance_batches / (it + 1))\n",
    "        ratio = np.max(\n",
    "            np.max(std, axis=0) / (values.max(axis=0) - values.min(axis=0)))\n",
    "\n",
    "        # Print progress message.\n",
    "        if verbose:\n",
    "            if detect_convergence:\n",
    "                print(f'StdDev Ratio = {ratio:.4f} (Converge at {thresh:.4f})')\n",
    "            else:\n",
    "                print(f'StdDev Ratio = {ratio:.4f}')\n",
    "\n",
    "        # Check for convergence.\n",
    "        if detect_convergence:\n",
    "            if ratio < thresh:\n",
    "                if verbose:\n",
    "                    print('Detected convergence')\n",
    "\n",
    "                # Skip bar ahead.\n",
    "                if bar:\n",
    "                    bar.n = bar.total\n",
    "                    bar.refresh()\n",
    "                break\n",
    "\n",
    "        # Forecast number of iterations required.\n",
    "        if detect_convergence:\n",
    "            N_est = (it + 1) * (ratio / thresh) ** 2\n",
    "            if bar and not np.isnan(N_est):\n",
    "                bar.n = np.around((it + 1) / N_est, 4)\n",
    "                bar.refresh()\n",
    "        elif bar:\n",
    "            bar.update(batch_size)\n",
    "\n",
    "        # Save intermediate quantities.\n",
    "        if return_all:\n",
    "            val_list.append(values)\n",
    "            std_list.append(std)\n",
    "            if detect_convergence:\n",
    "                N_list.append(N_est)\n",
    "\n",
    "    # Return results.\n",
    "    if return_all:\n",
    "        # Dictionary for progress tracking.\n",
    "        iters = (\n",
    "            (np.arange(it + 1) + 1) * batch_size *\n",
    "            (1 + int(paired_sampling)))\n",
    "        tracking_dict = {\n",
    "            'values': val_list,\n",
    "            'std': std_list,\n",
    "            'iters': iters}\n",
    "        if detect_convergence:\n",
    "            tracking_dict['N_est'] = N_list\n",
    "\n",
    "        return ShapleyValues(values, std), tracking_dict\n",
    "    else:\n",
    "        return ShapleyValues(values, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXACT - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import chain, combinations\n",
    "\n",
    "class Estimator:\n",
    "    \"\"\"Class for any estimator.\"\"\"\n",
    "\n",
    "    def __init__(self, model, num_features):\n",
    "        \"\"\"Initialize with model.\"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def _explain(self, explicand, num_evals):\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, explicand, num_evals=100):\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Exact(Estimator):\n",
    "    \"\"\"Exact estimation (expoential in the number of features)\"\"\"\n",
    "\n",
    "    def __init__(self, model, num_features):\n",
    "\n",
    "        super().__init__(model, num_features)\n",
    "\n",
    "        self.features = set(range(num_features))\n",
    "\n",
    "    def _powerset(self, iterable):\n",
    "        s = list(iterable)\n",
    "        return chain.from_iterable(\n",
    "            combinations(s, r) for r in range(len(s) + 1)\n",
    "        )\n",
    "\n",
    "    def _subset_on_off(self, subset, feature):\n",
    "\n",
    "        subset_off = np.zeros(self.num_features)\n",
    "        if subset:\n",
    "            subset_off[np.array(subset)] = 1\n",
    "\n",
    "        subset_on = np.copy(subset_off)\n",
    "        subset_on[feature] = 1\n",
    "\n",
    "        return (subset_on.astype(\"bool\"), subset_off.astype(\"bool\"))\n",
    "\n",
    "    def _single_feature(self, feature, explicand, baselines, y):\n",
    "        \n",
    "        total_masked_on=[]\n",
    "        total_masked_off=[]\n",
    "        total_size=[]\n",
    "        for baseline in baselines:\n",
    "            if baseline.ndim == 1:\n",
    "                baseline = baseline[np.newaxis]\n",
    "            masked_samples = np.repeat(baseline, 2 ** self.num_features, 0)\n",
    "            # print('MASKED SAMPLES SHAPE',masked_samples.shape)\n",
    "            sizes = []\n",
    "\n",
    "            subsets = self._powerset(self.features - set([feature])) # 2**N-1 features\n",
    "            for i, subset in enumerate(subsets):\n",
    "\n",
    "                subset_on, subset_off = self._subset_on_off(subset, feature)\n",
    "                # print('SUBSET ON',subset_on)\n",
    "                # print('SUBSET OFF',subset_off)\n",
    "\n",
    "                sizes.append(subset_off.sum())\n",
    "\n",
    "                masked_samples[i, subset_on] = explicand[subset_on] # first half of the samples\n",
    "                masked_samples[2 ** (self.num_features - 1) + i, subset_off] = explicand[subset_off] # second half of the samples\n",
    "\n",
    "            total_masked_on.append(masked_samples[:2 ** (self.num_features - 1)])\n",
    "            total_masked_off.append(masked_samples[2 ** (self.num_features - 1):])\n",
    "            total_size.extend(sizes)\n",
    "        \n",
    "        total_masked_on = np.concatenate(total_masked_on, axis=0)\n",
    "        total_masked_off = np.concatenate(total_masked_off, axis=0)\n",
    "        total_size = np.array(total_size)\n",
    "        \n",
    "        # print('TOTAL MASKED ON SHAPE',total_masked_on.shape)\n",
    "        # print('TOTAL MASKED OFF SHAPE',total_masked_off.shape)\n",
    "        # print('TOTAL SIZE SHAPE',total_size.shape)\n",
    "\n",
    "        # Compute marginal contributions\n",
    "        weights = 1 / scipy.special.comb(\n",
    "            self.num_features - 1, total_size\n",
    "        )\n",
    "        weights /= self.num_features\n",
    "        # print('WEIGHTS SHAPE',weights.shape)\n",
    "        # preds = self.model(masked_samples)[:, y]\n",
    "        # print('PRED SHAPE',preds.shape)\n",
    "        # preds_on = preds[: 2 ** (self.num_features - 1)]\n",
    "        preds_on = self.model(total_masked_on)[:, y]\n",
    "        # print('PRED ON SHAPE',preds_on.shape)\n",
    "        # preds_off = preds[2 ** (self.num_features - 1) :]\n",
    "        preds_off = self.model(total_masked_off)[:, y]\n",
    "        # print('PRED OFF SHAPE',preds_off.shape)\n",
    "        deltas = weights * (preds_on - preds_off)\n",
    "\n",
    "        return deltas.sum()/len(baselines)\n",
    "\n",
    "    def _explain(self, explicand, baselines,y):\n",
    "\n",
    "        phi = np.zeros(explicand.shape)\n",
    "\n",
    "        for i in range(self.num_features):\n",
    "            phi[i] = self._single_feature(i, explicand, baselines,y)\n",
    "\n",
    "        return phi\n",
    "\n",
    "    def __call__(self, explicand, baselines, y):\n",
    "\n",
    "        return self._explain(explicand, baselines, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAPSAMPLING - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from shapreg import utils, games, stochastic_games\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def ShapleySampling(game,\n",
    "                    batch_size=512,\n",
    "                    detect_convergence=True,\n",
    "                    thresh=0.01,\n",
    "                    n_samples=None,\n",
    "                    antithetical=False,\n",
    "                    return_all=False,\n",
    "                    bar=True,\n",
    "                    verbose=False):\n",
    "    # Verify arguments.\n",
    "    if isinstance(game, CooperativeGame):\n",
    "        stochastic = False\n",
    "    elif isinstance(game, StochasticCooperativeGame):\n",
    "        stochastic = True\n",
    "    else:\n",
    "        raise ValueError('game must be CooperativeGame or '\n",
    "                         'StochasticCooperativeGame')\n",
    "\n",
    "    # Possibly force convergence detection.\n",
    "    if n_samples is None:\n",
    "        n_samples = 1e20\n",
    "        if not detect_convergence:\n",
    "            detect_convergence = True\n",
    "            if verbose:\n",
    "                print('Turning convergence detection on')\n",
    "\n",
    "    if detect_convergence:\n",
    "        assert 0 < thresh < 1\n",
    "\n",
    "    # Calculate null coalition value.\n",
    "    if stochastic:\n",
    "        null = game.null(batch_size=batch_size)\n",
    "    else:\n",
    "        null = game.null()\n",
    "\n",
    "    # Set up bar.\n",
    "    n_loops = int(np.ceil(n_samples / batch_size))\n",
    "    if bar:\n",
    "        if detect_convergence:\n",
    "            bar = tqdm(total=1)\n",
    "        else:\n",
    "            bar = tqdm(total=n_loops * batch_size)\n",
    "\n",
    "    # Setup.\n",
    "    num_players = game.players\n",
    "    if isinstance(null, np.ndarray):\n",
    "        values = np.zeros((num_players, len(null)))\n",
    "        sum_squares = np.zeros((num_players, len(null)))\n",
    "        deltas = np.zeros((batch_size, num_players, len(null)))\n",
    "    else:\n",
    "        values = np.zeros((num_players))\n",
    "        sum_squares = np.zeros((num_players))\n",
    "        deltas = np.zeros((batch_size, num_players))\n",
    "    permutations = np.tile(np.arange(game.players), (batch_size, 1))\n",
    "    arange = np.arange(batch_size)\n",
    "    n = 0\n",
    "\n",
    "    # For tracking progress.\n",
    "    if return_all:\n",
    "        N_list = []\n",
    "        std_list = []\n",
    "        val_list = []\n",
    "\n",
    "    # Begin sampling.\n",
    "    for it in range(n_loops):\n",
    "        for i in range(batch_size):\n",
    "            if antithetical and i % 2 == 1:\n",
    "                permutations[i] = permutations[i - 1][::-1]\n",
    "            else:\n",
    "                np.random.shuffle(permutations[i])\n",
    "        S = np.zeros((batch_size, game.players), dtype=bool)\n",
    "\n",
    "        # Sample exogenous (if applicable).\n",
    "        if stochastic:\n",
    "            U = game.sample(batch_size)\n",
    "\n",
    "        # Unroll permutations.\n",
    "        prev_value = null\n",
    "        for i in range(num_players):\n",
    "            S[arange, permutations[:, i]] = 1\n",
    "            if stochastic:\n",
    "                next_value = game(S, U)\n",
    "            else:\n",
    "                next_value = game(S)\n",
    "            deltas[arange, permutations[:, i]] = next_value - prev_value\n",
    "            prev_value = next_value\n",
    "\n",
    "        # Welford's algorithm.\n",
    "        n += batch_size\n",
    "        diff = deltas - values\n",
    "        values += np.sum(diff, axis=0) / n\n",
    "        diff2 = deltas - values\n",
    "        sum_squares += np.sum(diff * diff2, axis=0)\n",
    "\n",
    "        # Calculate progress.\n",
    "        var = sum_squares / (n ** 2)\n",
    "        std = np.sqrt(var)\n",
    "        ratio = np.max(\n",
    "            np.max(std, axis=0) / (values.max(axis=0) - values.min(axis=0)))\n",
    "\n",
    "        # Print progress message.\n",
    "        if verbose:\n",
    "            if detect_convergence:\n",
    "                print(f'StdDev Ratio = {ratio:.4f} (Converge at {thresh:.4f})')\n",
    "            else:\n",
    "                print(f'StdDev Ratio = {ratio:.4f}')\n",
    "\n",
    "        # Check for convergence.\n",
    "        if detect_convergence:\n",
    "            if ratio < thresh:\n",
    "                if verbose:\n",
    "                    print('Detected convergence')\n",
    "\n",
    "                # Skip bar ahead.\n",
    "                if bar:\n",
    "                    bar.n = bar.total\n",
    "                    bar.refresh()\n",
    "                break\n",
    "\n",
    "        # Forecast number of iterations required.\n",
    "        if detect_convergence:\n",
    "            N_est = (it + 1) * (ratio / thresh) ** 2\n",
    "            if bar and not np.isnan(N_est):\n",
    "                bar.n = np.around((it + 1) / N_est, 4)\n",
    "                bar.refresh()\n",
    "        elif bar:\n",
    "            bar.update(batch_size)\n",
    "\n",
    "        # Save intermediate quantities.\n",
    "        if return_all:\n",
    "            val_list.append(np.copy(values))\n",
    "            std_list.append(np.copy(std))\n",
    "            if detect_convergence:\n",
    "                N_list.append(N_est)\n",
    "\n",
    "    # Return results.\n",
    "    if return_all:\n",
    "        # Dictionary for progress tracking.\n",
    "        iters = (np.arange(it + 1) + 1) * batch_size * num_players\n",
    "        tracking_dict = {\n",
    "            'values': val_list,\n",
    "            'std': std_list,\n",
    "            'iters': iters}\n",
    "        if detect_convergence:\n",
    "            tracking_dict['N_est'] = N_list\n",
    "\n",
    "        return ShapleyValues(values, std), tracking_dict\n",
    "    else:\n",
    "        return ShapleyValues(values, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepExplainer - Code (TF support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explainer(object):\n",
    "    \"\"\" This is the superclass of all explainers.\n",
    "    \"\"\"\n",
    "\n",
    "    def shap_values(self, X):\n",
    "        raise Exception(\"SHAP values not implemented for this explainer!\")\n",
    "\n",
    "    def attributions(self, X):\n",
    "        return self.shap_values(X)\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "# from shap.explainers.explainer import Explainer\n",
    "from distutils.version import LooseVersion\n",
    "keras = None\n",
    "tf = None\n",
    "tf_ops = None\n",
    "tf_gradients_impl = None\n",
    "\n",
    "class TFDeepExplainer(Explainer):\n",
    "    \"\"\"\n",
    "    Using tf.gradients to implement the backgropagation was\n",
    "    inspired by the gradient based implementation approach proposed by Ancona et al, ICLR 2018. Note\n",
    "    that this package does not currently use the reveal-cancel rule for ReLu units proposed in DeepLIFT.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data, session=None, learning_phase_flags=None):\n",
    "        \"\"\" An explainer object for a deep model using a given background dataset.\n",
    "\n",
    "        Note that the complexity of the method scales linearly with the number of background data\n",
    "        samples. Passing the entire training dataset as `data` will give very accurate expected\n",
    "        values, but be unreasonably expensive. The variance of the expectation estimates scale by\n",
    "        roughly 1/sqrt(N) for N background data samples. So 100 samples will give a good estimate,\n",
    "        and 1000 samples a very good estimate of the expected values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : keras.Model or (input : [tf.Operation], output : tf.Operation)\n",
    "            A keras model object or a pair of TensorFlow operations (or a list and an op) that\n",
    "            specifies the input and output of the model to be explained. Note that SHAP values\n",
    "            are specific to a single output value, so you get an explanation for each element of\n",
    "            the output tensor (which must be a flat rank one vector).\n",
    "\n",
    "        data : [numpy.array] or [pandas.DataFrame] or function\n",
    "            The background dataset to use for integrating out features. DeepExplainer integrates\n",
    "            over all these samples for each explanation. The data passed here must match the input\n",
    "            operations given to the model. If a function is supplied, it must be a function that\n",
    "            takes a particular input example and generates the background dataset for that example\n",
    "        session : None or tensorflow.Session\n",
    "            The TensorFlow session that has the model we are explaining. If None is passed then\n",
    "            we do our best to find the right session, first looking for a keras session, then\n",
    "            falling back to the default TensorFlow session.\n",
    "\n",
    "        learning_phase_flags : None or list of tensors\n",
    "            If you have your own custom learning phase flags pass them here. When explaining a prediction\n",
    "            we need to ensure we are not in training mode, since this changes the behavior of ops like\n",
    "            batch norm or dropout. If None is passed then we look for tensors in the graph that look like\n",
    "            learning phase flags (this works for Keras models). Note that we assume all the flags should\n",
    "            have a value of False during predictions (and hence explanations).\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # warnings.warn(\n",
    "        #     \"Please keep in mind DeepExplainer is brand new, and we are still developing it and working on \" +\n",
    "        #     \"characterizing/testing it on large networks. This means you should keep an eye out for odd \" +\n",
    "        #     \"behavior. Post any issues you run into on github.\"\n",
    "        # )\n",
    "\n",
    "        # try and import keras and tensorflow\n",
    "        global tf, tf_ops, tf_gradients_impl\n",
    "        if tf is None:\n",
    "            from tensorflow.python.framework import ops as tf_ops # pylint: disable=E0611\n",
    "            from tensorflow.python.ops import gradients_impl as tf_gradients_impl # pylint: disable=E0611\n",
    "            if not hasattr(tf_gradients_impl, \"_IsBackpropagatable\"):\n",
    "                from tensorflow.python.ops import gradients_util as tf_gradients_impl\n",
    "            import tensorflow as tf\n",
    "            if LooseVersion(tf.__version__) < LooseVersion(\"1.4.0\"):\n",
    "                warnings.warn(\"Your TensorFlow version is older than 1.4.0 and not supported.\")\n",
    "        global keras\n",
    "        if keras is None:\n",
    "            try:\n",
    "                import keras\n",
    "                if LooseVersion(keras.__version__) < LooseVersion(\"2.1.0\"):\n",
    "                    warnings.warn(\"Your Keras version is older than 2.1.0 and not supported.\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # determine the model inputs and outputs\n",
    "        if str(type(model)).endswith(\"keras.engine.sequential.Sequential'>\"):\n",
    "            self.model_inputs = model.inputs\n",
    "            self.model_output = model.layers[-1].output\n",
    "        elif str(type(model)).endswith(\"keras.models.Sequential'>\"):\n",
    "            self.model_inputs = model.inputs\n",
    "            self.model_output = model.layers[-1].output\n",
    "        elif str(type(model)).endswith(\"keras.engine.training.Model'>\"):\n",
    "            self.model_inputs = model.inputs\n",
    "            self.model_output = model.layers[-1].output\n",
    "        elif str(type(model)).endswith(\"tuple'>\"):\n",
    "            self.model_inputs = model[0]\n",
    "            self.model_output = model[1]\n",
    "        else:\n",
    "            assert False, str(type(model)) + \" is not currently a supported model type!\"\n",
    "        assert type(self.model_output) != list, \"The model output to be explained must be a single tensor!\"\n",
    "        assert len(self.model_output.shape) < 3, \"The model output must be a vector or a single value!\"\n",
    "        self.multi_output = True\n",
    "        if len(self.model_output.shape) == 1:\n",
    "            self.multi_output = False\n",
    "\n",
    "        # check if we have multiple inputs\n",
    "        self.multi_input = True\n",
    "        if type(self.model_inputs) != list or len(self.model_inputs) == 1:\n",
    "            self.multi_input = False\n",
    "            if type(self.model_inputs) != list:\n",
    "                self.model_inputs = [self.model_inputs]\n",
    "        if type(data) != list and (hasattr(data, '__call__')==False):\n",
    "            data = [data]\n",
    "        self.data = data\n",
    "        \n",
    "        self._vinputs = {} # used to track what op inputs depends on the model inputs\n",
    "        self.orig_grads = {}\n",
    "        \n",
    "        # if we are not given a session find a default session\n",
    "        if session is None:\n",
    "            # if keras is installed and already has a session then use it\n",
    "            ksess = None\n",
    "            if hasattr(keras.backend.tensorflow_backend, \"_SESSION\"):\n",
    "                ksess = keras.backend.tensorflow_backend._SESSION\n",
    "            elif hasattr(keras.backend.tensorflow_backend.tf_keras_backend._SESSION, \"session\"):\n",
    "                ksess = keras.backend.tensorflow_backend.tf_keras_backend._SESSION.session\n",
    "            if keras is not None and ksess is not None:\n",
    "                session = keras.backend.get_session()\n",
    "            else:\n",
    "                try:\n",
    "                    session = tf.compat.v1.keras.backend.get_session()\n",
    "                except:\n",
    "                    session = tf.keras.backend.get_session()\n",
    "        self.session = tf.get_default_session() if session is None else session\n",
    "\n",
    "        # if no learning phase flags were given we go looking for them\n",
    "        # ...this will catch the one that keras uses\n",
    "        # we need to find them since we want to make sure learning phase flags are set to False\n",
    "        if learning_phase_flags is None:\n",
    "            self.learning_phase_ops = []\n",
    "            for op in self.session.graph.get_operations():\n",
    "                if 'learning_phase' in op.name and op.type == \"Const\" and len(op.outputs[0].shape) == 0:\n",
    "                    if op.outputs[0].dtype == tf.bool:\n",
    "                        self.learning_phase_ops.append(op)\n",
    "            self.learning_phase_flags = [op.outputs[0] for op in self.learning_phase_ops]\n",
    "        else:\n",
    "            self.learning_phase_ops = [t.op for t in learning_phase_flags]\n",
    "\n",
    "        # save the expected output of the model\n",
    "        # if self.data is a function, set self.expected_value to None\n",
    "        if (hasattr(self.data, '__call__')):\n",
    "            self.expected_value = None\n",
    "        else:\n",
    "            if self.data[0].shape[0] > 5000:\n",
    "                warnings.warn(\"You have provided over 5k background samples! For better performance consider using smaller random sample.\")\n",
    "            self.expected_value = self.run(self.model_output, self.model_inputs, self.data).mean(0)\n",
    "\n",
    "        # find all the operations in the graph between our inputs and outputs\n",
    "        tensor_blacklist = tensors_blocked_by_false(self.learning_phase_ops) # don't follow learning phase branches\n",
    "        dependence_breakers = [k for k in op_handlers if op_handlers[k] == break_dependence]\n",
    "        back_ops = backward_walk_ops(\n",
    "            [self.model_output.op], tensor_blacklist,\n",
    "            dependence_breakers\n",
    "        )\n",
    "        self.between_ops = forward_walk_ops(\n",
    "            [op for input in self.model_inputs for op in input.consumers()],\n",
    "            tensor_blacklist, dependence_breakers,\n",
    "            within_ops=back_ops\n",
    "        )\n",
    "\n",
    "        # save what types are being used\n",
    "        self.used_types = {}\n",
    "        for op in self.between_ops:\n",
    "            self.used_types[op.type] = True\n",
    "\n",
    "        # make a blank array that will get lazily filled in with the SHAP value computation\n",
    "        # graphs for each output. Lazy is important since if there are 1000 outputs and we\n",
    "        # only explain the top 5 it would be a waste to build graphs for the other 995\n",
    "        if not self.multi_output:\n",
    "            self.phi_symbolics = [None]\n",
    "        else:\n",
    "            noutputs = self.model_output.shape.as_list()[1]\n",
    "            if noutputs is not None:\n",
    "                self.phi_symbolics = [None for i in range(noutputs)]\n",
    "            else:\n",
    "                raise Exception(\"The model output tensor to be explained cannot have a static shape in dim 1 of None!\")\n",
    "\n",
    "    def _variable_inputs(self, op):\n",
    "        \"\"\" Return which inputs of this operation are variable (i.e. depend on the model inputs).\n",
    "        \"\"\"\n",
    "        if op.name not in self._vinputs:\n",
    "            self._vinputs[op.name] = np.array([t.op in self.between_ops or t in self.model_inputs for t in op.inputs])\n",
    "        return self._vinputs[op.name]\n",
    "\n",
    "    def phi_symbolic(self, i):\n",
    "        \"\"\" Get the SHAP value computation graph for a given model output.\n",
    "        \"\"\"\n",
    "        if self.phi_symbolics[i] is None:\n",
    "\n",
    "            # replace the gradients for all the non-linear activations\n",
    "            # we do this by hacking our way into the registry (TODO: find a public API for this if it exists)\n",
    "            reg = tf_ops._gradient_registry._registry\n",
    "            for n in op_handlers:\n",
    "                if n in reg:\n",
    "                    self.orig_grads[n] = reg[n][\"type\"]\n",
    "                    if op_handlers[n] is not passthrough:\n",
    "                        reg[n][\"type\"] = self.custom_grad\n",
    "                elif n in self.used_types:\n",
    "                    raise Exception(n + \" was used in the model but is not in the gradient registry!\")\n",
    "            # In TensorFlow 1.10 they started pruning out nodes that they think can't be backpropped\n",
    "            # unfortunately that includes the index of embedding layers so we disable that check here\n",
    "            if hasattr(tf_gradients_impl, \"_IsBackpropagatable\"):\n",
    "                orig_IsBackpropagatable = tf_gradients_impl._IsBackpropagatable\n",
    "                tf_gradients_impl._IsBackpropagatable = lambda tensor: True\n",
    "            \n",
    "            # define the computation graph for the attribution values using custom a gradient-like computation\n",
    "            try:\n",
    "                out = self.model_output[:,i] if self.multi_output else self.model_output\n",
    "                self.phi_symbolics[i] = tf.gradients(out, self.model_inputs)\n",
    "\n",
    "            finally:\n",
    "\n",
    "                # reinstate the backpropagatable check\n",
    "                if hasattr(tf_gradients_impl, \"_IsBackpropagatable\"):\n",
    "                    tf_gradients_impl._IsBackpropagatable = orig_IsBackpropagatable\n",
    "\n",
    "                # restore the original gradient definitions\n",
    "                for n in op_handlers:\n",
    "                    if n in reg:\n",
    "                        reg[n][\"type\"] = self.orig_grads[n]\n",
    "        return self.phi_symbolics[i]\n",
    "\n",
    "    def shap_values(self, X, ranked_outputs=None, output_rank_order=\"max\", check_additivity=True):\n",
    "\n",
    "        # check if we have multiple inputs\n",
    "        if not self.multi_input:\n",
    "            if type(X) == list and len(X) != 1:\n",
    "                assert False, \"Expected a single tensor as model input!\"\n",
    "            elif type(X) != list:\n",
    "                X = [X]\n",
    "        else:\n",
    "            assert type(X) == list, \"Expected a list of model inputs!\"\n",
    "        assert len(self.model_inputs) == len(X), \"Number of model inputs (%d) does not match the number given (%d)!\" % (len(self.model_inputs), len(X))\n",
    "\n",
    "        # rank and determine the model outputs that we will explain\n",
    "        if ranked_outputs is not None and self.multi_output:\n",
    "            model_output_values = self.run(self.model_output, self.model_inputs, X)\n",
    "            if output_rank_order == \"max\":\n",
    "                model_output_ranks = np.argsort(-model_output_values)\n",
    "            elif output_rank_order == \"min\":\n",
    "                model_output_ranks = np.argsort(model_output_values)\n",
    "            elif output_rank_order == \"max_abs\":\n",
    "                model_output_ranks = np.argsort(np.abs(model_output_values))\n",
    "            else:\n",
    "                assert False, \"output_rank_order must be max, min, or max_abs!\"\n",
    "            model_output_ranks = model_output_ranks[:,:ranked_outputs]\n",
    "        else:\n",
    "            model_output_ranks = np.tile(np.arange(len(self.phi_symbolics)), (X[0].shape[0], 1))\n",
    "\n",
    "        # compute the attributions\n",
    "        output_phis = []\n",
    "        for i in range(model_output_ranks.shape[1]):\n",
    "            phis = []\n",
    "            for k in range(len(X)):\n",
    "                phis.append(np.zeros(X[k].shape))\n",
    "            for j in range(X[0].shape[0]):\n",
    "                if (hasattr(self.data, '__call__')):\n",
    "                    bg_data = self.data([X[l][j] for l in range(len(X))])\n",
    "                    if type(bg_data) != list:\n",
    "                        bg_data = [bg_data]\n",
    "                else:\n",
    "                    bg_data = self.data\n",
    "                # tile the inputs to line up with the background data samples\n",
    "                tiled_X = [np.tile(X[l][j:j+1], (bg_data[l].shape[0],) + tuple([1 for k in range(len(X[l].shape)-1)])) for l in range(len(X))]\n",
    "                # we use the first sample for the current sample and the rest for the references\n",
    "                joint_input = [np.concatenate([tiled_X[l], bg_data[l]], 0) for l in range(len(X))]\n",
    "                # run attribution computation graph\n",
    "                feature_ind = model_output_ranks[j,i]\n",
    "                sample_phis = self.run(self.phi_symbolic(feature_ind), self.model_inputs, joint_input)\n",
    "\n",
    "                # assign the attributions to the right part of the output arrays\n",
    "                for l in range(len(X)):\n",
    "                    phis[l][j] = (sample_phis[l][bg_data[l].shape[0]:] * (X[l][j] - bg_data[l])).mean(0)\n",
    "\n",
    "            output_phis.append(phis[0] if not self.multi_input else phis)\n",
    "        \n",
    "        if check_additivity:\n",
    "            self.expected_value = self.run(self.model_output, self.model_inputs, self.data).mean(0)\n",
    "            model_output = self.run(self.model_output, self.model_inputs, X)\n",
    "            for l in range(len(X)):\n",
    "                diffs = model_output[:, l] - self.expected_value[l] - output_phis[l].sum(axis=tuple(range(1, output_phis[l].ndim)))\n",
    "                assert np.abs(diffs).max() < 1e-4, \"Explanations do not sum up to the model's output! Please post as a github issue.\"\n",
    "        if not self.multi_output:\n",
    "            return output_phis[0]\n",
    "        elif ranked_outputs is not None:\n",
    "            return output_phis, model_output_ranks\n",
    "        else:\n",
    "            return output_phis\n",
    "\n",
    "    def run(self, out, model_inputs, X):\n",
    "        \"\"\" Runs the model while also setting the learning phase flags to False.\n",
    "        \"\"\"\n",
    "        feed_dict = dict(zip(model_inputs, X))\n",
    "        for t in self.learning_phase_flags:\n",
    "            feed_dict[t] = False\n",
    "        return self.session.run(out, feed_dict)\n",
    "\n",
    "    def custom_grad(self, op, *grads):\n",
    "        \"\"\" Passes a gradient op creation request to the correct handler.\n",
    "        \"\"\"\n",
    "        return op_handlers[op.type](self, op, *grads)\n",
    "\n",
    "\n",
    "def tensors_blocked_by_false(ops):\n",
    "    blocked = []\n",
    "    def recurse(op):\n",
    "        if op.type == \"Switch\":\n",
    "            blocked.append(op.outputs[1]) # the true path is blocked since we assume the ops we trace are False\n",
    "        else:\n",
    "            for out in op.outputs:\n",
    "                for c in out.consumers():\n",
    "                    recurse(c)\n",
    "    for op in ops:\n",
    "        recurse(op)\n",
    "\n",
    "    return blocked\n",
    "\n",
    "def backward_walk_ops(start_ops, tensor_blacklist, op_type_blacklist):\n",
    "    found_ops = []\n",
    "    op_stack = [op for op in start_ops]\n",
    "    while len(op_stack) > 0:\n",
    "        op = op_stack.pop()\n",
    "        if op.type not in op_type_blacklist and op not in found_ops:\n",
    "            found_ops.append(op)\n",
    "            for input in op.inputs:\n",
    "                if input not in tensor_blacklist:\n",
    "                    op_stack.append(input.op)\n",
    "    return found_ops\n",
    "\n",
    "def forward_walk_ops(start_ops, tensor_blacklist, op_type_blacklist, within_ops):\n",
    "    found_ops = []\n",
    "    op_stack = [op for op in start_ops]\n",
    "    while len(op_stack) > 0:\n",
    "        op = op_stack.pop()\n",
    "        if op.type not in op_type_blacklist and op in within_ops and op not in found_ops:\n",
    "            found_ops.append(op)\n",
    "            for out in op.outputs:\n",
    "                if out not in tensor_blacklist:\n",
    "                    for c in out.consumers():\n",
    "                        op_stack.append(c)\n",
    "    return found_ops\n",
    "\n",
    "\n",
    "def softmax(explainer, op, *grads):\n",
    "\n",
    "    in0 = op.inputs[0]\n",
    "    in0_max = tf.reduce_max(in0, axis=-1, keepdims=True, name=\"in0_max\")\n",
    "    in0_centered = in0 - in0_max\n",
    "    evals = tf.exp(in0_centered, name=\"custom_exp\")\n",
    "    rsum = tf.reduce_sum(evals, axis=-1, keepdims=True)\n",
    "    div = evals / rsum\n",
    "    explainer.between_ops.extend([evals.op, rsum.op, div.op, in0_centered.op]) # mark these as in-between the inputs and outputs\n",
    "    out = tf.gradients(div, in0_centered, grad_ys=grads[0])[0]\n",
    "    del explainer.between_ops[-4:]\n",
    "\n",
    "    # rescale to account for our shift by in0_max (which we did for numerical stability)\n",
    "    xin0,rin0 = tf.split(in0, 2)\n",
    "    xin0_centered,rin0_centered = tf.split(in0_centered, 2)\n",
    "    delta_in0 = xin0 - rin0\n",
    "    dup0 = [2] + [1 for i in delta_in0.shape[1:]]\n",
    "    return tf.where(\n",
    "        tf.tile(tf.abs(delta_in0), dup0) < 1e-6,\n",
    "        out,\n",
    "        out * tf.tile((xin0_centered - rin0_centered) / delta_in0, dup0)\n",
    "    )\n",
    "\n",
    "def maxpool(explainer, op, *grads):\n",
    "    xin0,rin0 = tf.split(op.inputs[0], 2)\n",
    "    xout,rout = tf.split(op.outputs[0], 2)\n",
    "    delta_in0 = xin0 - rin0\n",
    "    dup0 = [2] + [1 for i in delta_in0.shape[1:]]\n",
    "    cross_max = tf.maximum(xout, rout)\n",
    "    diffs = tf.concat([cross_max - rout, xout - cross_max], 0)\n",
    "    xmax_pos,rmax_pos = tf.split(explainer.orig_grads[op.type](op, grads[0] * diffs), 2)\n",
    "    return tf.tile(tf.where(\n",
    "        tf.abs(delta_in0) < 1e-7,\n",
    "        tf.zeros_like(delta_in0),\n",
    "        (xmax_pos + rmax_pos) / delta_in0\n",
    "    ), dup0)\n",
    "\n",
    "def gather(explainer, op, *grads):\n",
    "    #params = op.inputs[0]\n",
    "    indices = op.inputs[1]\n",
    "    #axis = op.inputs[2]\n",
    "    var = explainer._variable_inputs(op)\n",
    "    if var[1] and not var[0]:\n",
    "        assert len(indices.shape) == 2, \"Only scalar indices supported right now in GatherV2!\"\n",
    "\n",
    "        xin1,rin1 = tf.split(tf.to_float(op.inputs[1]), 2)\n",
    "        xout,rout = tf.split(op.outputs[0], 2)\n",
    "        dup_in1 = [2] + [1 for i in xin1.shape[1:]]\n",
    "        dup_out = [2] + [1 for i in xout.shape[1:]]\n",
    "        delta_in1_t = tf.tile(xin1 - rin1, dup_in1)\n",
    "        out_sum = tf.reduce_sum(grads[0] * tf.tile(xout - rout, dup_out), list(range(len(indices.shape), len(grads[0].shape))))\n",
    "        if op.type == \"ResourceGather\":\n",
    "            return [None, tf.where(\n",
    "                tf.abs(delta_in1_t) < 1e-6,\n",
    "                tf.zeros_like(delta_in1_t),\n",
    "                out_sum / delta_in1_t\n",
    "            )]\n",
    "        return [None, tf.where(\n",
    "            tf.abs(delta_in1_t) < 1e-6,\n",
    "            tf.zeros_like(delta_in1_t),\n",
    "            out_sum / delta_in1_t\n",
    "        ), None]\n",
    "    elif var[0] and not var[1]:\n",
    "        return [explainer.orig_grads[op.type](op, grads[0]), None] # linear in this case\n",
    "    else:\n",
    "        assert False, \"Axis not yet supported to be varying for gather op!\"\n",
    "\n",
    "def linearity_1d_nonlinearity_2d(input_ind0, input_ind1, op_func):\n",
    "    def handler(explainer, op, *grads):\n",
    "        var = explainer._variable_inputs(op)\n",
    "        if var[input_ind0] and not var[input_ind1]:\n",
    "            return linearity_1d_handler(input_ind0, explainer, op, *grads)\n",
    "        elif var[input_ind1] and not var[input_ind0]:\n",
    "            return linearity_1d_handler(input_ind1, explainer, op, *grads)\n",
    "        elif var[input_ind0] and var[input_ind1]:\n",
    "            return nonlinearity_2d_handler(input_ind0, input_ind1, op_func, explainer, op, *grads)\n",
    "        else:\n",
    "            return [None for _ in op.inputs] # no inputs vary, we must be hidden by a switch function\n",
    "    return handler\n",
    "\n",
    "def nonlinearity_1d_nonlinearity_2d(input_ind0, input_ind1, op_func):\n",
    "    def handler(explainer, op, *grads):\n",
    "        var = explainer._variable_inputs(op)\n",
    "        if var[input_ind0] and not var[input_ind1]:\n",
    "            return nonlinearity_1d_handler(input_ind0, explainer, op, *grads)\n",
    "        elif var[input_ind1] and not var[input_ind0]:\n",
    "            return nonlinearity_1d_handler(input_ind1, explainer, op, *grads)\n",
    "        elif var[input_ind0] and var[input_ind1]:\n",
    "            return nonlinearity_2d_handler(input_ind0, input_ind1, op_func, explainer, op, *grads)\n",
    "        else: \n",
    "            return [None for _ in op.inputs] # no inputs vary, we must be hidden by a switch function\n",
    "    return handler\n",
    "\n",
    "def nonlinearity_1d(input_ind):\n",
    "    def handler(explainer, op, *grads):\n",
    "        return nonlinearity_1d_handler(input_ind, explainer, op, *grads)\n",
    "    return handler\n",
    "\n",
    "def nonlinearity_1d_handler(input_ind, explainer, op, *grads):\n",
    "\n",
    "    # make sure only the given input varies\n",
    "    for i in range(len(op.inputs)):\n",
    "        if i != input_ind:\n",
    "            assert not explainer._variable_inputs(op)[i], str(i) + \"th input to \" + op.name + \" cannot vary!\"\n",
    "    \n",
    "    xin0,rin0 = tf.split(op.inputs[input_ind], 2)\n",
    "    xout,rout = tf.split(op.outputs[input_ind], 2)\n",
    "    delta_in0 = xin0 - rin0\n",
    "    dup0 = [2] + [1 for i in delta_in0.shape[1:]]\n",
    "    out = [None for _ in op.inputs]\n",
    "    orig_grads = explainer.orig_grads[op.type](op, grads[0])\n",
    "    out[input_ind] = tf.where(\n",
    "        tf.tile(tf.abs(delta_in0), dup0) < 1e-6,\n",
    "        orig_grads[input_ind] if len(op.inputs) > 1 else orig_grads,\n",
    "        grads[0] * tf.tile((xout - rout) / delta_in0, dup0)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def nonlinearity_2d_handler(input_ind0, input_ind1, op_func, explainer, op, *grads):\n",
    "    assert input_ind0 == 0 and input_ind1 == 1, \"TODO: Can't yet handle double inputs that are not first!\"\n",
    "    xout,rout = tf.split(op.outputs[0], 2)\n",
    "    xin0,rin0 = tf.split(op.inputs[input_ind0], 2)\n",
    "    xin1,rin1 = tf.split(op.inputs[input_ind1], 2)\n",
    "    delta_in0 = xin0 - rin0\n",
    "    delta_in1 = xin1 - rin1\n",
    "    dup0 = [2] + [1 for i in delta_in0.shape[1:]]\n",
    "    out10 = op_func(xin0, rin1)\n",
    "    out01 = op_func(rin0, xin1)\n",
    "    out11,out00 = xout,rout\n",
    "    out0 = 0.5 * (out11 - out01 + out10 - out00)\n",
    "    out0 = grads[0] * tf.tile(out0 / delta_in0, dup0)\n",
    "    out1 = 0.5 * (out11 - out10 + out01 - out00)\n",
    "    out1 = grads[0] * tf.tile(out1 / delta_in1, dup0)\n",
    "\n",
    "    # see if due to broadcasting our gradient shapes don't match our input shapes\n",
    "    if (np.any(np.array(out1.shape) != np.array(delta_in1.shape))):\n",
    "        broadcast_index = np.where(np.array(out1.shape) != np.array(delta_in1.shape))[0][0]\n",
    "        out1 = tf.reduce_sum(out1, axis=broadcast_index, keepdims=True)\n",
    "    elif (np.any(np.array(out0.shape) != np.array(delta_in0.shape))):\n",
    "        broadcast_index = np.where(np.array(out0.shape) != np.array(delta_in0.shape))[0][0]\n",
    "        out0 = tf.reduce_sum(out0, axis=broadcast_index, keepdims=True)\n",
    "\n",
    "    # Avoid divide by zero nans\n",
    "    out0 = tf.where(tf.abs(tf.tile(delta_in0, dup0)) < 1e-7, tf.zeros_like(out0), out0)\n",
    "    out1 = tf.where(tf.abs(tf.tile(delta_in1, dup0)) < 1e-7, tf.zeros_like(out1), out1)\n",
    "\n",
    "    return [out0, out1]\n",
    "\n",
    "def linearity_1d(input_ind):\n",
    "    def handler(explainer, op, *grads):\n",
    "        return linearity_1d_handler(input_ind, explainer, op, *grads)\n",
    "    return handler\n",
    "\n",
    "def linearity_1d_handler(input_ind, explainer, op, *grads):\n",
    "    # make sure only the given input varies (negative means only that input cannot vary, and is measured from the end of the list)\n",
    "    for i in range(len(op.inputs)):\n",
    "        if i != input_ind:\n",
    "            assert not explainer._variable_inputs(op)[i], str(i) + \"th input to \" + op.name + \" cannot vary!\"\n",
    "    return explainer.orig_grads[op.type](op, *grads)\n",
    "\n",
    "def linearity_with_excluded(input_inds):\n",
    "    def handler(explainer, op, *grads):\n",
    "        return linearity_with_excluded_handler(input_inds, explainer, op, *grads)\n",
    "    return handler\n",
    "\n",
    "def linearity_with_excluded_handler(input_inds, explainer, op, *grads):\n",
    "    # make sure the given inputs don't vary (negative is measured from the end of the list)\n",
    "    for i in range(len(op.inputs)):\n",
    "        if i in input_inds or i - len(op.inputs) in input_inds:\n",
    "            assert not explainer._variable_inputs(op)[i], str(i) + \"th input to \" + op.name + \" cannot vary!\"\n",
    "    return explainer.orig_grads[op.type](op, *grads)\n",
    "\n",
    "def passthrough(explainer, op, *grads):\n",
    "    return explainer.orig_grads[op.type](op, *grads)\n",
    "\n",
    "def break_dependence(explainer, op, *grads):\n",
    "    \"\"\" This function name is used to break attribution dependence in the graph traversal.\n",
    "     \n",
    "    These operation types may be connected above input data values in the graph but their outputs\n",
    "    don't depend on the input values (for example they just depend on the shape).\n",
    "    \"\"\"\n",
    "    return [None for _ in op.inputs]\n",
    "\n",
    "\n",
    "op_handlers = {}\n",
    "\n",
    "# ops that are always linear\n",
    "op_handlers[\"Identity\"] = passthrough\n",
    "op_handlers[\"StridedSlice\"] = passthrough\n",
    "op_handlers[\"Squeeze\"] = passthrough\n",
    "op_handlers[\"ExpandDims\"] = passthrough\n",
    "op_handlers[\"Pack\"] = passthrough\n",
    "op_handlers[\"BiasAdd\"] = passthrough\n",
    "op_handlers[\"Unpack\"] = passthrough\n",
    "op_handlers[\"Add\"] = passthrough\n",
    "op_handlers[\"Sub\"] = passthrough\n",
    "op_handlers[\"Merge\"] = passthrough\n",
    "op_handlers[\"Sum\"] = passthrough\n",
    "op_handlers[\"Mean\"] = passthrough\n",
    "op_handlers[\"Cast\"] = passthrough\n",
    "op_handlers[\"Transpose\"] = passthrough\n",
    "op_handlers[\"Enter\"] = passthrough\n",
    "op_handlers[\"Exit\"] = passthrough\n",
    "op_handlers[\"NextIteration\"] = passthrough\n",
    "op_handlers[\"Tile\"] = passthrough\n",
    "op_handlers[\"TensorArrayScatterV3\"] = passthrough\n",
    "op_handlers[\"TensorArrayReadV3\"] = passthrough\n",
    "op_handlers[\"TensorArrayWriteV3\"] = passthrough\n",
    "\n",
    "# ops that don't pass any attributions to their inputs\n",
    "op_handlers[\"Shape\"] = break_dependence\n",
    "op_handlers[\"RandomUniform\"] = break_dependence\n",
    "op_handlers[\"ZerosLike\"] = break_dependence\n",
    "#op_handlers[\"StopGradient\"] = break_dependence # this allows us to stop attributions when we want to (like softmax re-centering)\n",
    "\n",
    "# ops that are linear and only allow a single input to vary\n",
    "op_handlers[\"Reshape\"] = linearity_1d(0)\n",
    "op_handlers[\"Pad\"] = linearity_1d(0)\n",
    "op_handlers[\"ReverseV2\"] = linearity_1d(0)\n",
    "op_handlers[\"ConcatV2\"] = linearity_with_excluded([-1])\n",
    "op_handlers[\"Conv2D\"] = linearity_1d(0)\n",
    "op_handlers[\"Switch\"] = linearity_1d(0)\n",
    "op_handlers[\"AvgPool\"] = linearity_1d(0)\n",
    "op_handlers[\"FusedBatchNorm\"] = linearity_1d(0)\n",
    "\n",
    "# ops that are nonlinear and only allow a single input to vary\n",
    "op_handlers[\"Relu\"] = nonlinearity_1d(0)\n",
    "op_handlers[\"Elu\"] = nonlinearity_1d(0)\n",
    "op_handlers[\"Sigmoid\"] = nonlinearity_1d(0)\n",
    "op_handlers[\"Tanh\"] = nonlinearity_1d(0)\n",
    "op_handlers[\"Softplus\"] = nonlinearity_1d(0)\n",
    "op_handlers[\"Exp\"] = nonlinearity_1d(0)\n",
    "op_handlers[\"ClipByValue\"] = nonlinearity_1d(0)\n",
    "op_handlers[\"Rsqrt\"] = nonlinearity_1d(0)\n",
    "op_handlers[\"Square\"] = nonlinearity_1d(0)\n",
    "op_handlers[\"Max\"] = nonlinearity_1d(0)\n",
    "\n",
    "# ops that are nonlinear and allow two inputs to vary\n",
    "op_handlers[\"SquaredDifference\"] = nonlinearity_1d_nonlinearity_2d(0, 1, lambda x, y: (x - y) * (x - y))\n",
    "op_handlers[\"Minimum\"] = nonlinearity_1d_nonlinearity_2d(0, 1, lambda x, y: tf.minimum(x, y))\n",
    "op_handlers[\"Maximum\"] = nonlinearity_1d_nonlinearity_2d(0, 1, lambda x, y: tf.maximum(x, y))\n",
    "\n",
    "# ops that allow up to two inputs to vary are are linear when only one input varies\n",
    "op_handlers[\"Mul\"] = linearity_1d_nonlinearity_2d(0, 1, lambda x, y: x * y)\n",
    "op_handlers[\"RealDiv\"] = linearity_1d_nonlinearity_2d(0, 1, lambda x, y: x / y)\n",
    "op_handlers[\"MatMul\"] = linearity_1d_nonlinearity_2d(0, 1, lambda x, y: tf.matmul(x, y))\n",
    "\n",
    "# ops that need their own custom attribution functions\n",
    "op_handlers[\"GatherV2\"] = gather\n",
    "op_handlers[\"ResourceGather\"] = gather\n",
    "op_handlers[\"MaxPool\"] = maxpool\n",
    "op_handlers[\"Softmax\"] = softmax\n",
    "\n",
    "class DeepExplainer(Explainer):\n",
    "\n",
    "    def __init__(self, model, data, session=None, learning_phase_flags=None):\n",
    "        self.explainer = TFDeepExplainer(model, data, session, learning_phase_flags)\n",
    "        self.expected_value = self.explainer.expected_value\n",
    "\n",
    "    def shap_values(self, X, ranked_outputs=None, output_rank_order='max'):\n",
    "\n",
    "        return self.explainer.shap_values(X, ranked_outputs, output_rank_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloIter(IND, DATA, MODEL, y):\n",
    "    svs=[]\n",
    "    x=DATA.iloc[IND]\n",
    "    TH=0\n",
    "    M=1000\n",
    "    error_sv_m=0\n",
    "    svs_iter=[]\n",
    "    for j in range(num_features):\n",
    "        # calculate the shaply value for feature j\n",
    "        n_features = len(x)\n",
    "        marginal_contributions = []\n",
    "        feature_idxs = list(range(n_features))\n",
    "        feature_idxs.remove(j)\n",
    "        feat_iter=[]\n",
    "        for iter in range(M):\n",
    "            z = DATA.sample(1).values[0]\n",
    "            x_idx = random.sample(feature_idxs, min(max(int(0.2*n_features), random.choice(feature_idxs)), int(0.8*n_features)))\n",
    "            z_idx = [idx for idx in feature_idxs if idx not in x_idx]\n",
    "            \n",
    "            # construct two new instances\n",
    "            x_plus_j = np.array([x[i] if i in x_idx + [j] else z[i] for i in range(n_features)])\n",
    "            x_minus_j = np.array([z[i] if i in z_idx + [j] else x[i] for i in range(n_features)])\n",
    "            \n",
    "            # calculate marginal contribution\n",
    "            marginal_contribution = MODEL.predict(x_plus_j.reshape(1, -1))[0][y] - MODEL.predict(x_minus_j.reshape(1, -1))[0][y] ## ADAPT\n",
    "            marginal_contributions.append(marginal_contribution)\n",
    "            feat_iter.append(sum(marginal_contributions) / len(marginal_contributions))\n",
    "            # if iter%32==0:\n",
    "            #     feat_iter.append(sum(marginal_contributions) / len(marginal_contributions))\n",
    "            \n",
    "        phi_j_x = sum(marginal_contributions) / len(marginal_contributions)  # our shaply value\n",
    "        svs.append(phi_j_x)\n",
    "        svs_iter.append(feat_iter)\n",
    "     \n",
    "    # change svs_iter such that the final dimension is MxNumFeatures\n",
    "    svs_iter=np.array(svs_iter)\n",
    "    return svs, svs_iter.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DASP.dasp.dasp import DASP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=[]\n",
    "fs=[]\n",
    "ls=[]\n",
    "ksv=[]\n",
    "psv=[]\n",
    "ssv=[]\n",
    "msv=[]\n",
    "esv=[]\n",
    "daspsv=[]\n",
    "deepsv=[]\n",
    "gradsv=[]\n",
    "\n",
    "df_X_Test_s=pd.DataFrame(X_test_s)\n",
    "df_X_train_s=pd.DataFrame(X_train_s)\n",
    "\n",
    "# pexplainer = shap.explainers.Permutation(bbm_model.predict_proba, df_X_train_s)\n",
    "# sexplainer = shap.explainers.Sampling(bbm_model.predict_proba, df_X_train_s)\n",
    "# eexplainer = shap.explainers.Exact(bbm_model.predict_proba, X_train_s)\n",
    "# imputer = MarginalExtension(X_test_s[:128], bbm_model.predict_proba)\n",
    "# exact = Exact(bbm_model.predict_proba, num_features)\n",
    "\n",
    "exact = Exact(bbmodel.predict, num_features)\n",
    "eexplainer = shap.explainers.Exact(bbmodel.predict, X_train_s)\n",
    "imputer = MarginalExtension(X_test_s[:128], bbmodel.predict)\n",
    "exact = Exact(bbmodel.predict, num_features)\n",
    "\n",
    "deepexpl = DeepExplainer(bbmodel, X_train_s[:1000])\n",
    "gradexpl = shap.GradientExplainer(bbmodel, X_train_s[:1000])\n",
    "daspex = DASP(Model(bbmodel.inputs, bbmodel.layers[-2].output))\n",
    "# deep = DeepShap(bbm_model, X_train_s, num_features)\n",
    "\n",
    "time_fs=[]\n",
    "time_ls=[]  \n",
    "time_sr=[]\n",
    "time_mc=[]\n",
    "time_psv=[]\n",
    "time_ssv=[]\n",
    "time_ex=[]\n",
    "time_dasp=[]\n",
    "time_deep=[]\n",
    "time_grad=[]\n",
    "\n",
    "unbiased_kernelshap_curves = []\n",
    "kernelshap_curves = []\n",
    "# list_iters=np.arange(64, 1024+1, 64)\n",
    "UPPER=800\n",
    "if UPPER==800:\n",
    "    TAKE=25\n",
    "    TAKE2=80\n",
    "else:\n",
    "    TAKE=32\n",
    "    TAKE2=103\n",
    "\n",
    "list_iters=np.arange(32, UPPER+1, 32)\n",
    "perm_iters=np.arange(10, UPPER+1, 10)\n",
    "mc_iters=np.arange(1, UPPER+1, 1)\n",
    "print(num_features)\n",
    "print(len(list_iters), len(perm_iters), len(mc_iters))\n",
    "\n",
    "# imputer = MarginalExtension(X_test_s[:128], bbm_model.predict_proba)\n",
    "imputer = MarginalExtension(X_test_s[:128], bbmodel.predict)\n",
    "thresh = 0.01\n",
    "samples = 12228\n",
    "\n",
    "l2_distance_fs=[]\n",
    "l2_distance_ls=[]\n",
    "l2_distance_ks=[]\n",
    "l2_distance_mc=[]\n",
    "l2_distance_uks=[]\n",
    "l2_distance_psv=[]\n",
    "l2_distance_dasp=[]\n",
    "l2_distance_deep=[]\n",
    "l2_distance_grad=[]\n",
    "\n",
    "kernelshap_iters=128\n",
    "\n",
    "for ind in tqdm(range(25)):\n",
    "    x = X_test_s[ind:ind+1]\n",
    "    y = int(Y_test[ind])\n",
    "\n",
    "    # set all seeds\n",
    "    SEED = 42\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # Run FastSHAP\n",
    "    start = time.time()\n",
    "    fastshap_values = fastshap.shap_values(x)[0]\n",
    "    end = time.time()\n",
    "    time_fs.append(end-start)\n",
    "    fsv=fastshap_values[:, y]\n",
    "\n",
    "    # Run LightningSHAP\n",
    "    start = time.time()\n",
    "    lshap_values=lshap.shap_values(x)[0]\n",
    "    end = time.time()\n",
    "    time_ls.append(end-start)\n",
    "    lsv=lshap_values[:, y]\n",
    "    \n",
    "    # Run KernelSHAP and Unbiased KernelSHAP\n",
    "    game = PredictionGame(imputer, x)\n",
    "    start = time.time()\n",
    "    # uks, ks = ShapleyRegression(game, batch_size=32, thresh=thresh, bar=False, paired_sampling=True, return_all=True)\n",
    "    uks, ks = ShapleyRegression(game, batch_size=32, thresh=thresh, bar=False, paired_sampling=False, return_all=True)\n",
    "    end = time.time()\n",
    "    time_sr.append(end-start)\n",
    "    # target_uks=np.array([ el[:,y] for el in uks['values']])[:16,:]\n",
    "    # target_ks=np.array([ el[:,y] for el in ks['values']])[:16,:]\n",
    "    # target_uks=np.array([ el[:,y] for el in uks['values']])[:TAKE,:]\n",
    "    target_uks=uks.values[:,y]\n",
    "    # target_ks=np.array([ el[:,y] for el in ks['values']])[:TAKE,:]\n",
    "    target_ks=ks['values'][list(ks['iters']).index(kernelshap_iters)][:, y]\n",
    "    # print(target_uks.shape)\n",
    "    # print(target_ks.shape)\n",
    "    \n",
    "\n",
    "    # Run Permutation\n",
    "    start = time.time()\n",
    "    # results = ShapleySampling(game, batch_size=1, n_samples=int(np.ceil(samples / num_features)), detect_convergence=False, bar=False, antithetical=True, return_all=True)\n",
    "    \n",
    "    \n",
    "    results = ShapleySampling(game, batch_size=1, n_samples=int(np.ceil(samples / num_features)), detect_convergence=False, bar=False, return_all=True)\n",
    "    \n",
    "    # print(results[1]['values'][-1][:,y])\n",
    "    # break\n",
    "    end = time.time()\n",
    "    time_psv.append(end-start)\n",
    "    # permutation = np.array([explanation[:,y] for explanation in results[1]['values']])[:-1,:]\n",
    "    permutation = results[1]['values'][-1][:,y]\n",
    "\n",
    "    # Run MonteCarlo\n",
    "    start = time.time()\n",
    "    mc, mc_iter= MonteCarloIter(ind, df_X_train_s, bbmodel, y)\n",
    "    end = time.time()\n",
    "    time_mc.append(end-start)\n",
    "    # print(mc_iter.shape)\n",
    "\n",
    "    # Run DASP\n",
    "    start = time.time()\n",
    "    dasp_values = daspex.run(x, num_features)\n",
    "    end = time.time()\n",
    "    time_dasp.append(end-start)\n",
    "    daspv=dasp_values[0][y]\n",
    "\n",
    "    # Run DeepExplainer\n",
    "    start = time.time()\n",
    "    deep_values = deepexpl.shap_values(x)\n",
    "    end = time.time()\n",
    "    time_deep.append(end-start)\n",
    "    deepv=deep_values[y][0]\n",
    "\n",
    "    # Run GradientExplainer\n",
    "    start = time.time()\n",
    "    grad_values = gradexpl.shap_values(x)\n",
    "    end = time.time()\n",
    "    time_grad.append(end-start)\n",
    "    gradv=grad_values[y][0]\n",
    "\n",
    "    # Run Exact SHAP\n",
    "    if num_features>=15:\n",
    "        ev=target_uks\n",
    "    else:\n",
    "        start = time.time()\n",
    "        # ev = eexplainer(x).values[0][:,y]\n",
    "        ev = exact(x[0], X_test_s[:128], y)\n",
    "        end = time.time()\n",
    "        time_ex.append(end-start)\n",
    "\n",
    "    ts.append(target_uks)\n",
    "    ksv.append(target_ks)\n",
    "    fs.append(fsv)\n",
    "    ls.append(lsv)\n",
    "    psv.append(permutation)\n",
    "    msv.append(mc)\n",
    "    esv.append(ev)\n",
    "    daspsv.append(daspv)\n",
    "    deepsv.append(deepv)\n",
    "    gradsv.append(gradv)\n",
    "\n",
    "    l2_distance_fs.append(np.linalg.norm(ev-fsv))\n",
    "    l2_distance_ls.append(np.linalg.norm(ev-lsv))\n",
    "\n",
    "    # l2_kshap=np.array([euclidean_dist(ev,el) for el in target_ks])\n",
    "    # l2_kshap=np.array([np.linalg.norm(ev-el) for el in target_ks])\n",
    "    # print(l2_kshap.shape)\n",
    "    l2_distance_ks.append(np.linalg.norm(ev-target_ks))\n",
    "\n",
    "    # l2_ukshap=np.array([np.linalg.norm(ev-el) for el in target_uks])\n",
    "    # print(l2_ukshap.shape)\n",
    "    l2_distance_uks.append(np.linalg.norm(ev-target_uks))\n",
    "\n",
    "    # l2_pshap=np.array([np.linalg.norm(ev-el) for el in permutation])\n",
    "    # print(l2_pshap.shape)\n",
    "    l2_distance_psv.append(np.linalg.norm(ev-permutation))\n",
    "\n",
    "    # l2_mcshap=np.array([np.linalg.norm(ev-el) for el in mc_iter])\n",
    "    # print(l2_mcshap.shape)\n",
    "    l2_distance_mc.append(np.linalg.norm(ev-mc))\n",
    "\n",
    "    l2_daspshap=np.linalg.norm(ev-daspv)\n",
    "    l2_distance_dasp.append(l2_daspshap)\n",
    "\n",
    "    l2_deepshap=np.linalg.norm(ev-deepv)\n",
    "    l2_distance_deep.append(l2_deepshap)\n",
    "\n",
    "    l2_gradshap=np.linalg.norm(ev-gradv)\n",
    "    l2_distance_grad.append(l2_gradshap)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Times\n",
    "\n",
    "print(\"Time taken by SR: {:.4f}\".format(np.mean(time_sr)))\n",
    "print(\"Time taken by Permutation: {:.4f}\".format(np.mean(time_psv)))\n",
    "print(\"Time taken by MonteCarlo: {:.4f}\".format(np.mean(time_mc)))\n",
    "print(\"Time taken by Exact: {:.4f}\".format(np.mean(time_ex)))\n",
    "print(\"Time taken by FastSHAP: {:.4f}\".format(np.mean(time_fs)))\n",
    "print(\"Time taken by LightningSHAP: {:.4f}\".format(np.mean(time_ls)))\n",
    "print(\"Time taken by DASP: {:.4f}\".format(np.mean(time_dasp)))\n",
    "print(\"Time taken by DeepExplainer: {:.4f}\".format(np.mean(time_deep)))\n",
    "print(\"Time taken by GradientExplainer: {:.4f}\".format(np.mean(time_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print L2 distances\n",
    "print(\"L2 distance between Exact and KernelSHAP: {:.4f}\".format(np.mean(l2_distance_ks)))\n",
    "print(\"L2 distance between Exact and Unbiased KernelSHAP: {:.4f}\".format(np.mean(l2_distance_uks)))\n",
    "print(\"L2 distance between Exact and Permutation: {:.4f}\".format(np.mean(l2_distance_psv)))\n",
    "print(\"L2 distance between Exact and MonteCarlo: {:.4f}\".format(np.mean(l2_distance_mc)))\n",
    "print(\"L2 distance between Exact and FastSHAP: {:.4f}\".format(np.mean(l2_distance_fs)))\n",
    "print(\"L2 distance between Exact and LightningSHAP: {:.4f}\".format(np.mean(l2_distance_ls)))\n",
    "print(\"L2 distance between Exact and DASP: {:.4f}\".format(np.mean(l2_distance_dasp)))\n",
    "print(\"L2 distance between Exact and DeepExplainer: {:.4f}\".format(np.mean(l2_distance_deep)))\n",
    "print(\"L2 distance between Exact and GradientExplainer: {:.4f}\".format(np.mean(l2_distance_grad)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
